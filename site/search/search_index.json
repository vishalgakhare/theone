{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Application and System Design with AWS cloud, lead teams to develop well-managed platforms. (Serverless Apps in JS, TS) - Developed multiple serverless apps and utilities for the telephony system to interact with Amazon Lex. Re-designed and developed a softphone application. 2018 (Java, NodeJS, Bash Script, Mongo DB) - Worked on Java Spark and NodeJS batches to tokenize ~50M accounts and migrate ~250M records. 2016 (NodeJS, MongoDB) - Designed and developed Rest Api services backed with Mongo DB. The Rest services serviced 2M requests per day at an average of 20ms response time. 2012 (C#.Net, VS2010)- Worked on CTIBHO (Computer Telephony Interface) to support Avaya Dialer upgrade to version 5. The project won the Enterprise Circle of Excellence award. 2008 (C#.Net, VS2008)- Worked on Wells Fargo + Wachovia merger project to optimize/tune the performance of the application and the Oracle database queries. The performance improved by 50% for Oracle DB and by 90 % for applications. 2006 (C#.Net, VS2005) - Developed a Verizon One Source search module single-handedly on all the waterfall phases and delivered it with zero bugs.","title":"Home"},{"location":"Docker/","text":"Docker Commands docker logs <container_id> docker stats <container_id> : If you just need to keep an eye on the metrics of your container to work out what\u2019s gone wrong, docker stats can help: it\u2019ll give you a live stream of resource usage, so you can see just how much memory you\u2019ve leaked so far. docker cp <container_id> : /path/to/useful/file/local-path Often just getting hold of more log files is enough to sort you out. If you already know what you want, docker cp has your back: copy any file from any container back out onto your local machine, so you can examine it in depth (especially useful analysing heap dumps)._ docker exec -it <container_id> /bin/bash : Next up, if you can run the container (if it\u2019s crashed, you can restart it with docker start ), shell in directly and start digging around for further details by hand. docker commit <container_id> my-broken-container && docker run -it my-broken-container /bin/bash","title":"Docker"},{"location":"Docker/#docker-commands","text":"docker logs <container_id> docker stats <container_id> : If you just need to keep an eye on the metrics of your container to work out what\u2019s gone wrong, docker stats can help: it\u2019ll give you a live stream of resource usage, so you can see just how much memory you\u2019ve leaked so far. docker cp <container_id> : /path/to/useful/file/local-path Often just getting hold of more log files is enough to sort you out. If you already know what you want, docker cp has your back: copy any file from any container back out onto your local machine, so you can examine it in depth (especially useful analysing heap dumps)._ docker exec -it <container_id> /bin/bash : Next up, if you can run the container (if it\u2019s crashed, you can restart it with docker start ), shell in directly and start digging around for further details by hand. docker commit <container_id> my-broken-container && docker run -it my-broken-container /bin/bash","title":"Docker Commands"},{"location":"AWS/Cheatsheet/","text":"EC2 Dedicated (Instances): No other customers will share the hardware. May share hardware with other instances of ONLY your account. (Dedicated) Hosts: Book an entire physical server and have full control of EC2 instance placement. You can only change the tenancy of an instance from dedicated to host or from host to dedicated after you\u2019ve launched it. Good EC2 combo -> reserved instances for baseline + on-demand & spot for peaks. Userdata Executed as root by default. Hibernate Hibernation saves the contents from the instance memory (RAM) to your Amazon EBS root volume. When you start your instance: The Amazon EBS root volume is restored to its previous state. The RAM contents are reloaded. EC2 Hibernation To use hibernation, the root volume must be an encrypted EBS volume. When the instance state is stopping, you will not be billed if it is preparing to stop. However, you will still be billed if it is just preparing to hibernate. Spot instances A Spot Instance request is either one-time or persistent. If the spot request is persistent, the request is opened again after your Spot Instance is interrupted. Spot blocks are Spot Instances with a defined duration & are designed not to be interrupted . If your Spot Instance request is disabled and has an associated stopped Spot Instance, canceling the request does not terminate the instance . Placement groups It is recommended that you launch the number of instances that you need in the placement group in a single launch request and that you use the same instance type for all instances in the placement group. If you try to add more instances to the placement group later, or if you try to launch more than one instance type in the placement group, you increase your chances of getting an insufficient capacity error. If you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group and try the launch again. Restarting the instances may migrate them to hardware that has the capacity for all the requested instances. Spread Maximum of 7 running instances per Availability Zone per group. Recommended for applications that have a small number of critical instances that should be kept separate from each other . Spread placement groups provide access to distinct racks and are therefore suitable for mixing instance types or launching instances over time. Cluster Higher per-flow throughput limit of up to 10 Gbps for TCP/IP traffic and are placed in the same high-bisection bandwidth segment of the network. Partition Spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions . Used by l arge distributed and replicated workloads . Autoscaling Lifecycle hooks enable you to perform custom actions as the Auto Scaling group launches or terminates instances. EC2 Autoscaling Lifecycle Hooks Lifecycle hooks put the instance into a wait state until the script or timeout period ends . With launch templates, you can provision capacity across multiple instance types using both On-Demand Instances and Spot Instances. You can put an instance that is in the InService state into the Standby state, update some software or troubleshoot the instance, and then return the instance to service. Auto Scaling doesn\u2019t terminate an instance that came into service based on EC2 status checks and ELB health checks until the health check grace period expires . Cooldown period: It ensures that the Auto Scaling group does not launch or terminate additional EC2 instances before the previous scaling activity takes effect(default 300s). Amazon EC2 Auto Scaling does not immediately terminate instances with an Impaired status . By default, Amazon EC2 Auto Scaling doesn\u2019t use the results of ELB health checks to determine an instance\u2019s health status when the group\u2019s health check configuration is set to EC2. When there are multiple policies in force at the same time, Auto Scaling chooses the policy that provides the largest capacity for both scale-out and scale-in . The default value for the instance placement tenancy is null, and the instance tenancy is controlled by the tenancy attribute of the VPC. If you set the Launch Configuration Tenancy to default and the VPC Tenancy is set to dedicate, then the instances have dedicated tenancy. If you set the Launch Configuration Tenancy to dedicated and the VPC Tenancy is set to default, then again, the instances have dedicated tenancy. If you have an EC2 Auto Scaling group (ASG) with running instances and you choose to delete the ASG, the instances will be terminated, and the ASG will be deleted . Rebalancing AZs launches new instances before terminating the old ones. Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance. S3 S3 standard: There is no minimum storage duration charge and no retrieval fee (use case: if you want to keep data for a few days only) Object-level permissions: For actions inside the bucket (e.g. GetObject), add / after arn, -> arn:aws:s3:::test/ With bucket policies, you can grant users within your AWS Account or other AWS Accounts access to your Amazon S3 resources. The AWS S3 sync command uses the CopyObject APIs to copy objects between S3 buckets. By default, S3 replication only supports copying new Amazon S3 objects after it is enabled. Max upload 5GB per time , for more use multi-part upload. If the object to upload is > 100 MB , you should consider using multipart uploads . Amazon S3 delivers strong read-after-write consistency automatically. You can increase your read or write performance by parallelizing reads with prefixes. Once you version-enable a bucket, it can never return to an unversioned state. Versioning can only be suspended once it has been enabled. No S3 data transfer charges when data is transferred in from the internet. Also, with S3TA, you pay only for transfers that are accelerated . Using the Range HTTP header in a GET Object request, you can fetch a byte range from an object, transferring only the specified portion. A byte-range request is a perfect way to get the beginning of a file . You can place a retention period on an object version. Different versions of a single object can have different retention modes and periods. Max object size 5TB . For replication must enable versioning in source and destination. By default, an S3 object is owned by the AWS account that uploaded it, even in a bucket in a different account. To get full access to the object, the object owner must explicitly grant the bucket owner access. You can create a bucket policy to require external users to grant bucket-owner-full-control when uploading objects so the bucket owner can have full access to the objects. Object lock: store objects as locked(only on versioned buckets). Metadata , which can be included with the object, is not encrypted while being stored on Amazon S3. Therefore, AWS recommends that customers not place sensitive information in Amazon S3 metadata. S3 event notification allows destinations: SQS standard, Lambda, SNS . Allowed names for S3 website endpoints: http://bucket-name.s3-website.Region.amazonaws.com & http://bucket-name.s3-website-Region.amazonaws.com S3 Select scan a subset of an object by specifying a range of bytes to query based on the bucket\u2019s name and the object\u2019s key. With S3 Select, you can use simple structured query language (SQL) statements to filter the contents of an Amazon S3 object and retrieve just the subset of data that you need. CSV, JSON, or Apache Parquet format. S3 can publish notifications for the following events: New object-created events, Object removal events, Restore object events, Reduced Redundancy Storage (RRS) object lost events, Replication events . To encrypt an object at the time of upload , you need to add a header called x-amz-server-side-encryption . To enforce object encryption, create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption header. To enable S3 website: a) An S3 bucket that is configured to host a static website. The bucket must have the same name as your domain or subdomain, b) a registered domain name c) Route 53 as the DNS service for the domain. S3 server access logs provide detailed records for the requests that are made to an S3 bucket. 3,500 requests per second to add data and 5,500 requests per second to retrieve data. You can have an S3 bucket that has different objects stored in S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA. S3 IA S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. The minimum storage duration is 30 days before you can transition objects from S3 Standard to S3 Standard IA or One Zone-IA.(This limitation does not apply to Intelligent Tiering, Glacier, and Glacier Deep Archive) S3 Lifecycle Transitions Supported lifecycle transitions \u2014 waterfall model : The S3 Standard storage class to any other storage class. Any storage class to the S3 Glacier or S3 Glacier Deep Archive storage classes. The S3 Standard-IA storage class to the S3 Intelligent-Tiering or S3 One Zone-IA storage classes. The S3 Intelligent-Tiering storage class to the S3 One Zone-IA storage class. The S3 Glacier storage class to the S3 Glacier Deep Archive storage class. S3 Lifecycle Transitions Encrypted objects remain encrypted throughout the storage class transition process . Glacier Glacier supports encryption by default for both data at rest as well as in transit. The minimal storage duration period is 90 days for the S3 Glacier storage class and 180 days for S3 Glacier Deep Archive . Data can be stored directly in Amazon S3 Glacier Deep Archive. Snowball Snowball Edge storage optimised: 80TB 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity. You can\u2019t directly copy data from Snowball Edge devices into AWS Glacier. For data < 10PB or distributed in multiple locations . Snowball Edge compute optimised(52 vCPUs, 42 TB of usable block or object storage, and an optional GPU). Snowball Edge possibility for storage clustering. AWS OpsHub is a graphical user interface you can use to manage your AWS Snowball devices. Snowmobile Each Snowmobile has a total capacity of up to 100 petabytes. For data > 10PB in a single location . Snow Services Comparison IAM Permissions Boundary to limit max access of users . They can only be applied to roles or users, not IAM groups. IAM Permissions Boundaries IAM Policy Evaluation Logic: if there is an explicit deny, the final decision is to deny for the resource. When you assume a role, you give up your original permissions and take the permissions of the assigned role. When using a resource-based policy, the principal doesn\u2019t have to give up his permissions. In a policy condition: aws:RequestedRegion represents the target of the API call. You can share an AMI with another account. Trust Policy: only IAM resource-based policy. If you got your certificate from a third-party CA, import the certificate into ACM or upload it to the IAM certificate store. With web identity federation , you don\u2019t need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. Security Token Service(STS) Temporary security credentials that can control access to your AWS resources. AWS Organizations It does not offer the federation capability. To migrate an account to another Organization: remove member account, send an invite to new Org, Accept the invite to the new Org from the member account. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization\u2019s access control guidelines. SCPs affect all users and roles in the attached accounts, including the root user. SCPs do not affect any service-linked role. VPC VPN connection: Virtual Private Gateway endpoint on the AWS VPC side \u2014 Customer Gateway on the on-premises side. You can\u2019t have a VPC with only a public subnet and AWS Site-to-Site VPN. Private IPs allowed ranges: 10.0.0.0/8 (10.0.0.0\u201310.255.255.255), 172.16.0.0/12 (172.16.0.0\u2013172.31.255.255), 192.168.0.0/16(192.168.0.0\u2013192.168.255.255) AWS reserves 5 Ip addresses in each subnet . Shared services VPC, which provides access to services required by workloads in each of the VPCs. This might include directory services or VPC endpoints. Sharing resources from a central location instead of building them in each VPC may reduce administrative overhead and cost. Use AZ ID to uniquely identify the Availability Zones across the two AWS Accounts. By default, non-default subnets have the IPv4 public addressing(assign public IP) attribute set to false , and default subnets have this attribute set to true. You cannot disable IPv4 support for your VPC and subnets since this is the default IP addressing system for Amazon VPC and Amazon EC2. Every subnet that you create is automatically associated with the main route table for the VPC. Allowed block size in VPC is between a /16 netmask (65,536 IP addresses) and /28 netmask. While primary ENIs cannot be detached from an instance, secondary ENIs can be detached and attached to a different instance. Security Groups If nothing is defined in a security group, then all access is blocked. NACL NACLs are stateless, so outbound rules have to be evaluated again. Defined at Subnet level. Should allow outbound traffic from ephemeral ports. NACL rules are evaluated starting with the lowest numbered rule. As soon as a rule matches traffic, it\u2019s applied immediately regardless of any higher-numbered rule that may contradict it. Cloudhub Multiple AWS Site-to-Site VPN connections, you can provide secure communication between sites using the AWS VPN CloudHub, including Direct Connect connections. Supports IP Multicast. Low-cost primary or secondary network connectivity between locations, only for VPNs. Direct Connect Maximum resilience is achieved by separate connections terminating on separate devices in more than one location. Direct Connect High Resiliency Setup Dedicated connection 1\u201310 Gbps. Hosted connection 50Mbps -10Gbps, add or remove capacity on demand. Data in transit is not encrypted , but private. Transit Gateway Network transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. AWS Transit Gateway also enables you to scale the IPsec VPN throughput with equal-cost multi-path (ECMP) routing support over multiple VPN tunnels. NAT Instance It can be used as a bastion, supports security groups, supports port-forwarding, must disable ec2 flag source/destination check. Nat Gateway Only for IPv4. Set up in a public subnet. In a specific AZ, and can only be used by instances in other subnets. Egress-only Internet Gateway: nat for ipv6. Route53 Routing policy multi-value supports up to 8 healthy records for each multi-value query. To integrate an external domain to route53, update the nameservers on the 3rd party registrar with your public hosted zone. To resolve any DNS queries for resources in the AWS VPC from the on-premises network , you can create an inbound endpoint on Route 53 Resolver, and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint. To resolve DNS queries for any resources in the on-premises network from the AWS VPC , you can create an outbound endpoint on Route 53 Resolver, and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint. Cannot create a CNAME record for the top node of the DNS namespace. So, if you register the DNS name mpla.com the zone apex is mpla.com. You can\u2019t create a CNAME record for mpla.com, but you can create an alias record for mpla.com that routes traffic to www.mpla.com. Route 53 doesn\u2019t charge for alias queries to AWS resources, but Route 53 does charge for CNAME queries . For each VPC that you want to associate with the Route 53 hosted zone, change the following VPC settings to true: enableDnsHostnames , enableDnsSupport . You configure active-active failover using any routing policy (or combination of routing policies) other than failover, and you configure active-passive failover using the failover routing policy. Active-Active Failover when you want all of your resources to be available the majority of the time. Active-Passive Failover when you want a primary resource or group of resources to be available the majority of the time, and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable. EBS By default, the root volume for an AMI backed by Amazon EBS is deleted when the instance terminates. For an encrypted EBS volume, data stored at rest on the volume, data moving between the volume and the instance, snapshots created from the volume, and volumes created from those snapshots are all encrypted. GP2: system boot volumes, 1GB \u2014 16TB, max IOPS 16,000; if you add 1TB, you get +3000IOPS for low latency interactive apps. io1/io2: 4GB-16TB, max 64,000 IOPS, 50:1 IOPS:GB ratio. io2 Block Express volumes, Provisioned IOPS (PIOPS) up to 256,000 , with an IOPS:GiB ratio of 1,000:1, f or submillisecond latency for > 64,000 IOPS or 1000 MB/s throughput. Throughput optimised HDD(st1): max throughput 500 MB/s \u2014 max 500 IOPS \u2014 Big data, log processing, data warehouses. Cold HDD(scl): max throughput 250 MB/s \u2014 max 250 IOPS \u2014 Throughput-oriented storage that is infrequently accessed, low storage cost scenarios. Amazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD (io1 or io2) volume to multiple instances with Nitro system that are in the same Availability Zone. Throughput Optimized HDD ( st1 ) and Cold HDD ( sc1 ) volume types cannot be used for boot volumes . EBS Types Comparison Locked to AZ, to attach to other AZ, you have to snapshot it. Copying an unencrypted snapshot allows encryption. When copying an AMI to another region, it automatically creates the underlying EBS snapshot also in the new region. RAID 0 to increase performance. RAID 1 to increase fault tolerance. If the instance is already running, you can set DeleteOnTermination to False using the command line for the root EBS volume. An in-progress snapshot is not affected by ongoing reads and writes to the volume; hence, you can still use the EBS volume normally. Enforce the encryption of the new EBS volumes and snapshot copies that you create with the Encryption by Default feature(no effect on existing EBS volumes or snapshots). If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region. When you enable encryption by default, you can launch an instance only if the instance type supports EBS encryption. Amazon EBS does not support asymmetric CMKs. Instance Store Temporary block-level storage for your instance. Ideal for the temporary storage of information that frequently changes, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers Instance Store For high I/O performance, instance store volumes are a better option. You cant resize the instance store. EFS Control which EC2 instances can access your EFS file system with security group rules and IAM policies. 1000s on concurrent NFS clients, 10Gbs throughput. Use EFS Access Points to manage application access. Max I/O performance mode is used to scale to higher levels of aggregate throughput and operations per second \u2014 tradeoff of slightly higher latencies. General Purpose performance mode is ideal for latency-sensitive use cases. POSIX is compliant. Provisioned Throughput mode: for applications with high throughput to storage (MiB/s per TiB) ratios or with requirements greater than those allowed by the Bursting Throughput mode. Bursting Throughput mode: designed to burst to high throughput levels for periods of time. Higher price point than EBS . The maximum days for the EFS lifecycle policy is 90. Amazon FSx for Lustre: Run the world\u2019s most popular high-performance file system. For machine learning, high-performance computing (HPC), video processing, and financial modeling. Ability to both process the \u2018hot data\u2019 in a parallel and distributed fashion as well as easily store the \u2018cold data\u2019 on Amazon S3. RDS Multi A-Z synchronous replication across AZs. Replication between the primary and standby instances does not incur additional data transfer charges. Read replicas asynchronous replication across AZs or cross-region, up to 5. RDS Multi-AZ, Multi-region, Read Replicas Comparison Backups every 5min, ability to restore at any point in time. Supports storage autoscaling. IAM database authentication works with MySQL and PostgreSQL . Use an authentication token with a lifetime of 15 minutes. RDS provides metrics in real-time for the operating system (OS) that your DB instance runs on with Enhanced Monitoring (RDS processes, RDS child processes, OS processes). To encrypt unencrypted RDS database: create a snapshot of your DB instance, and then create an encrypted copy of that snapshot, restore DB from an encrypted snapshot, terminate the previous DB. Upgrades to the database engine level require downtime . Even if your RDS DB instance uses a Multi-AZ deployment, both the primary and standby DB instances are upgraded at the same time. This causes downtime until the upgrade is complete, and the duration of the downtime varies based on the size of your DB instance. RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary, and finally performing maintenance on the old primary, which becomes the new standby. The maximum backup retention period for automated backup is 35 days. Aurora Auto-scales up to 128 TB per database instance. Aurora cluster: one Primary DB instance \u2014 up to 15 replicas(read-only) . Aurora Cluster You can specify the failover priority for Aurora Replicas; each Read Replica is associated with a priority tier (0\u201315). Aurora will promote the Read Replica that has the highest priority (the lowest numbered tier). If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is the largest in size. Aurora Global Database is designed for globally distributed applications , allowing a single Amazon Aurora database to span multiple AWS regions, sub-second data access in any region . Storage automatically grows in increments of 10GB. In a multi-master cluster, all DB instances can perform write operations(scale writes, avoid downtime for writes) \u2014 continuous availability for applications where you can\u2019t afford even brief downtime for database write operations. Using endpoints, you can map each connection to the appropriate instance or group of instances based on your use case. For clusters with DB instances of different capacities or configurations, you can connect to custom endpoints associated with different subsets of DB instances. Reader endpoint automatically performs load-balancing among all the Aurora Replicas. For diagnosis or tuning, you can connect to a specific instance endpoint to examine details about a specific DB instance. If you are running Aurora Serverless and the DB instance or AZ becomes unavailable, Aurora will automatically recreate the DB instance in a different AZ. If you have a single instance, Aurora will attempt to create a new DB Instance in the same Availability Zone as the original instance. This replacement of the original instance is done on a best-effort basis and may not succeed. DynamoDB DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement \u2014 from milliseconds to microseconds . Tables must have provisioned read and write capacity units RCU, WRC. DynamoDB Streams allow changes in DynamoDB to be streamed to other services(read by Lambda etc., 24h retention on streams). Global Tables support multi-region replication, low latency, disaster recovery. Must first enable Streams. Can only query on the primary key, sort key, or indexes. All DynamoDB tables are encrypted . There is no option to enable or disable encryption for new or existing tables. By default, all DynamoDB tables are encrypted under an AWS owned customer master key (CMK), which does not write to CloudTrail logs. If the shard iterator expires immediately before you can use it, this might indicate that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data. To solve, increase the write capacity assigned to the shard table. Elasticache For sub-millisecond latency caching , ElastiCache is the best choice. Memcached Supports multithreaded architecture . Redis Redis HIPAA compliant , supports replication, high availability, and cluster sharding. The in-memory data store that provides sub-millisecond latency . IAM Auth is not supported by ElastiCache. Redis AUTH(enable Redis to require a token (password) before allowing clients to execute commands, thereby improving data security). Redshift With Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum For OLAP: online analytical processing. Redshift enhanced VPC routing; copy/unload goes through VPC. Possibility to copy snapshots for a cluster to another region for DR. Cloudwatch Metrics belong to namespaces; Dimension is an attribute of a metric, Up to 10 dimensions per metric. Automatically recover ec2: If your instance has a public IPv4 address, it retains the public IPv4 address after recovery. During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost . You can use CloudWatch Events to run Amazon ECS tasks when certain AWS events occur. EventBridge Recommended when you want to build an application that reacts to events from SaaS applications and/or AWS services. Only event-based service that integrates directly with third-party SaaS partners. Encryption/Secrets Key Policies: control access to keys; you cannot control access without them. Automatic key rotation: CMK every one year. SSE-KMS is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Deleting a customer master key (CMK) has enforced a waiting period; you schedule key deletion( minimum of 7 days up to a maximum of 30 days(default) ) SSE-C \u2014 With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys, and Amazon S3 manages the encryption, as it writes to disks and decryption when you access your objects. SSE-S3 \u2014 When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. Uses 256-bit Advanced Encryption Standard (AES-256). Client-side encryption when there is a proprietary encryption algorithm. Secrets Manager Nice RDS integration. Force secret rotation every X days. SSM Parameter Store Allow assigning TTL to a parameter(expiration date) to force update/delete of sensitive data. CloudHSM With dedicated hardware, you manage your own encryption keys. A good option to use with SSE-C. It is possible to lose keys that were created since the most recent daily backup if the CloudHSM cluster that you are using fails and you are not using two or more HSMs. Kinesis Kinesis Data Streams Default data retention 1 day can go up to 7. 1MB/sec/shard ingest capacity. By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream. Use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel, automatically scales throughput with the number of shards. The ability for multiple apps to consume the same stream concurrently. Ability to consume records in the same order a few hours later . Routing related records to the same record processor. For example, counting and aggregation are simpler when all records for a given key are routed to the same record processor. Kinesis Firehose It automatically scales to match the throughput of your data and requires no ongoing administration. The auto-scaling solution, as there is no need to provision any shards like Kinesis Data Streams. Kinesis Agent cannot write to a Kinesis Firehose for which the delivery stream source is already set as Kinesis Data Streams. Data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints and Datadog, New Relic, MongoDB, Splunk. Load streaming data into Redshift for near real-time analytics. SQS When you need messaging semantics (ack/fail) and visibility timeout (default 30s). Dynamically increasing concurrency/throughput at read time. FIFO queues support up to 3,000 messages (batch 10 messages per operation- max) per second with batching( 300 without ), have an 80-character queue name limit. Message retention 4 days default, 14 days max . Limit 256kb per message sent. To scale to the same number of consumers as producers , send data with a Group ID attribute . Delay queues let you postpone the delivery of new messages to a queue for several seconds. The default (minimum) delay for a queue is 0 seconds . The maximum is 15 minutes. You can use message timers to set an initial invisibility period for a message added to a queue. Default delay for a message in 0 seconds. The maximum is 15 minutes. Temporary queues help you save development time and deployment costs when using common message patterns such as request-response. To better support short-lived, lightweight messaging destinations, AWS recommends Amazon SQS Temporary Queue Client. The key concept behind the client is the V irtual Queue . Virtual queues let you multiplex many low-traffic queues onto a single SQS queue. SQS Virtual Queues AWS recommends using separate queues to provide prioritization of work . A single SQS message queue can contain an unlimited number of messages. However, there is a 120,000 quota for the number of inflight messages for a standard queue and 20,000 for a FIFO queue. Messages are inflight after they have been received from the queue by a consuming component but have not yet been deleted from the queue. Standard queues provide at least one delivery, which means that each message is delivered at least once. FIFO queues provide exactly-once processing, which means that each message is delivered once and remains available until a consumer processes it and deletes it. Duplicates are not introduced into the queue. An Amazon SQS message can contain up to 10 metadata attributes. By default, ReceiveMessageWaitTimeSeconds is zero, which means it is using Short polling. If it is set to a value greater than zero, then it is Long polling. SNS Event producers send events to 1 topic; we can have many subs. 100000 topics limit Use SNS message filtering to assign a filter policy to the topic subscription, and the subscriber will only receive a message that they are interested in . SNS FIFO for strict message ordering and deduplicated message delivery to one or more subscribers SNS FIFO LoadBalancers LBs can scale but not instantaneously. Elastic Load Balancing stops sending requests to targets that are deregistering. By default, Elastic Load Balancing waits 300s(can be set between 1s to 3600s) seconds before completing the deregistration process, which can help in-flight requests to the target to complete (connection drain). When cross-zone load balancing is enabled , each load balancer node distributes traffic across the registered targets in all enabled Availability Zones evenly . Cross-Zone Load Balancing Enabled By default, cross-zone load balancing is enabled for Application Load Balancer and disabled for Network Load Balancer. ELB cannot distribute incoming traffic for targets deployed in different regions. Access logging is an optional feature of Elastic Load Balancing that is disabled by default. Use to analyze traffic patterns and troubleshoot issues. Application LoadBalancer ALB targets with instance ID route to primary private IP in primary NIC targets using IP addresses route to any private IP from one or more NICs. Host-based Routing : You can route a client request based on the Host field of the HTTP header, allowing you to route to multiple domains from the same load balancer. Path-based Routing : You can route a client request based on the URL path of the HTTP header. HTTP header-based routing : You can route a client request based on the value of any standard or custom HTTP header. HTTP method-based routing : You can route a client request based on any standard or custom HTTP method. Query string parameter-based routing : You can route a client request based on the query string or query parameters. Source IP address CIDR-based routing : You can route a client request based on source IP address CIDR from where the request originates. ALB not registered any targets with the target groups -> 503 error. Use Cognito Authentication via Cognito User Pools for your ALB. With SNI support AWS makes it easy to use more than one certificate with the same ALB. You can host multiple TLS secured applications, each with its own TLS certificate, behind a single ALB. In order to use SNI, all you need to do is bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client. ALBs support Weighted Target Groups routing. Network LoadBalancer NLB traffic is routed using the private IP address. Network LB has no security groups, it lets traffic passing by. With Network Load Balancer (NLB), you can offload the decryption/encryption of TLS traffic from your application servers to the NLB. Classic LoadBalancer CLB does not support Server Name Indication (SNI). Lambda Supports 1000 concurrent executions per AWS account per region , contact support to raise the limit if needed. Supported languages: C#/.NET, GO, node.js, Python, Java, Ruby. lamda@edge: deploy lambda to each region alongside your CloudFront CDN. You can set your memory from 128MB to 10,240MB If your Lambda function accesses a VPC, you must make sure that your VPC has sufficient ENI or subnect IPs capacity to support the scale requirements of your Lambda function. Step Functions Serverless workflows orchestration. Cloudfront Delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users. Use CloudFront to improve application performance to serve static content from S3. Dynamic content does not flow through regional edge caches but goes directly to the origin \u2014 Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin. Preferred to handle spikes in traffic over GA. You cannot directly integrate Cognito User Pools with CloudFront distribution as you have to create a separate Lambda@Edge function to accomplish the authentication via Cognito User Pools. CloudFront can route to multiple origins based on the content type . Field-level encryption: The sensitive information provided by your users is encrypted at the edge (You can\u2019t encrypt all of the data in a request with field-level encryption; you must specify individual fields to encrypt). CloudFront signed cookies -> provide access to multiple restricted files . CloudFront signed URLs -> access to one file . You can also use an EC2 instance or a custom origin in configuring CloudFront. The Cache-Control and Expires headers control how long objects stay in the cache. The Cache-Control max-age directive lets you specify how long (in seconds) you want an object to remain in the cache before CloudFront gets the object again from the origin server. The minimum expiration time CloudFront supports is 0 seconds for web distributions and 3600 seconds for RTMP distributions. Global Accelerator(GA) Directs traffic to optimal endpoints over the AWS global network. Improves the availability and performance of your internet applications. Two static anycast IP addresses act as a fixed entry point to your application endpoints. Global Accelerator Overview Good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP. Uses endpoint weights to determine the proportion of traffic that is directed to endpoints in an endpoint group(can be used in blue/green deployments). Global Accelerator Weighted Endpoints WAF Use AWS WAF to block or allow requests based on conditions that you specify, such as the IP addresses. Geographic (Geo) Match Conditions in AWS WAF to restrict application access based on the geographic location of your viewers \u2014 choose the countries from which AWS WAF should allow access. Protects against SQL injection and Cross-Site Scripting . Rate based rules(DDoS protection). Firewall Manager Centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. You can centrally configure AWS WAF rules, AWS Shield Advanced protection, Amazon Virtual Private Cloud (VPC) security groups, AWS Network Firewalls, and Amazon Route 53 Resolver DNS Firewall rules across accounts and resources in your organization. It does not support Network ACLs as of today. AWS Shield DDoS , protection against SYN/UDP floods , reflection attacks, and other layer/3 & layer 4 attacks. EMR Cloud big data platform for processing vast amounts of data using Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto, Hadoop. Beanstalk Easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker. Automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. You retain full control over the AWS resources powering your application and can access the underlying resources at any time. Application files are stored in S3. The server log files can also optionally be stored in S3 or in CloudWatch Logs. CloudFormation StackSet extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Use the CreationPolicy attribute when you want to wait on resource configuration actions before stack creation proceeds. Cognito User pools Provide built-in user management e.g., sign-in and register functionality for apps. Identity pools Provide temporary credentials for AWS access to users. AWS Database Migration Service Seamlessly migrate data from supported sources to relational databases, data warehouses, streaming platforms, and other data stores in the AWS cloud. (e.g., quickly move data from S3 to Kinesis data streams, not only for DBs). Database Migration Service Use Case: Migrate data from S3 to Kinesis Storage Gateway File Gateway File Gateway Architecture SMB or NFS access to data in S3 with local caching. Volume Gateway Present cloud-based iSCSI block storage volumes to your on-premises applications. Tape Gateway Supports archiving directly to Glacier and Glacier Deep Archive. DataSync Move large data from on-premise to AWS. Can move data directly to Glacier or Glacier Deep Archive . AppSync Store and sync data across mobile and web apps in real-time. CloudTrail By default, CloudTrail event log files are encrypted using Amazon S3 server-side encryption (SSE). Xray AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. End-to-end view of requests as they travel through your application , and shows a map of your application\u2019s underlying components. The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. GuardDuty Threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. Analyses AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs . Disabling the service in the general settings deletes all the remaining data . Macie Discover and protect your sensitive data on Amazon S3. Inspector Helps you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. Recognition Automate your image and video analysis with machine learning. VPC Endpoints When you create a VPC endpoint, you can attach an endpoint policy that controls access to the service to which you are connecting. Gateway Endpoints GE is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service . S3 & DynamoDB only. Interface Endpoints An elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service. Enhanced Networking Elastic Network Adapter(ENA) Enhanced networking capabilities with network speeds of up to 100 Gbps. Supports Windows. Elastic Fabric Adapter (EFA) Network device that you can attach to your Amazon EC2 instance to accelerate High-Performance Computing (HPC) and machine learning applications. ENA with added capabilities. Doesn\u2019t support Windows. EFA support can be enabled either at the launch of the instance or added to a stopped instance. EFA devices cannot be attached to a running instance. API Gateway Rest APIs \u2014 stateful client-server communication. Websocket APIs \u2014 stateless full-duplex communication. All of the APIs created with Amazon API Gateway expose HTTPS endpoints only SWF Simple Workflow Service Use if you need: external signals to intervene , or child processes to return values to parent processes . For decoupled architectures. Provides useful guarantees around task assignments. It ensures that a task is never duplicated and is assigned only once. AWS Backup Centralized backup service. A backup plan is a policy expression that defines when and how you want to back up your AWS resources. AWS Batch Multi-node parallel jobs. AWS ParallelCluster Cluster management tool to deploy HPC, automate creation of vpc, subnet, cluster type etc. AD Connector If you only need to allow your on-premises users to log in to AWS applications and services with their Active Directory credentials. AWS Managed Microsoft AD Configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO). Data Transfer No charge for inbound data transfer across all services in all Regions. Data transfer from AWS to the internet is charged per service, with rates specific to the originating Region. There is a charge for data transfer across Regions. Data transfer within the same Availability Zone is free. Data transfer over a VPC peering connection that stays within an Availability Zone is free. Data transfer over a VPC peering connection that crosses Availability Zones will incur a data transfer charge for ingress/egress traffic. If the VPCs are peered across Regions, standard inter-Region data transfer charges will apply. Data processing charges apply for each GB sent from a VPC, Direct Connect, or VPN to Transit Gateway. Direct Connect & VPN also incur charges for data flowing out of AWS. Important ports: FTP: 21 SSH: 22 SFTP: 22 (same as SSH) HTTP: 80 HTTPS: 443 RDP: TCP 3389 and UDP 3389 RDS Databases ports: PostgreSQL: 5432 MySQL: 3306 Oracle RDS: 1521 MSSQL Server: 1433 MariaDB: 3306 (same as MySQL) Aurora: 5432 (if PostgreSQL compatible) or 3306 (if MySQL compatible) Disaster Recovery in AWS RPO: Recovery Point Objective -> how much data loss we are willing to recover RTO: Recovery Time Objective -> downtime between disaster and RTO","title":"Cheatsheet"},{"location":"AWS/Cheatsheet/#ec2","text":"Dedicated (Instances): No other customers will share the hardware. May share hardware with other instances of ONLY your account. (Dedicated) Hosts: Book an entire physical server and have full control of EC2 instance placement. You can only change the tenancy of an instance from dedicated to host or from host to dedicated after you\u2019ve launched it. Good EC2 combo -> reserved instances for baseline + on-demand & spot for peaks. Userdata Executed as root by default. Hibernate Hibernation saves the contents from the instance memory (RAM) to your Amazon EBS root volume. When you start your instance: The Amazon EBS root volume is restored to its previous state. The RAM contents are reloaded. EC2 Hibernation To use hibernation, the root volume must be an encrypted EBS volume. When the instance state is stopping, you will not be billed if it is preparing to stop. However, you will still be billed if it is just preparing to hibernate. Spot instances A Spot Instance request is either one-time or persistent. If the spot request is persistent, the request is opened again after your Spot Instance is interrupted. Spot blocks are Spot Instances with a defined duration & are designed not to be interrupted . If your Spot Instance request is disabled and has an associated stopped Spot Instance, canceling the request does not terminate the instance .","title":"EC2"},{"location":"AWS/Cheatsheet/#placement-groups","text":"It is recommended that you launch the number of instances that you need in the placement group in a single launch request and that you use the same instance type for all instances in the placement group. If you try to add more instances to the placement group later, or if you try to launch more than one instance type in the placement group, you increase your chances of getting an insufficient capacity error. If you receive a capacity error when launching an instance in a placement group that already has running instances, stop and start all of the instances in the placement group and try the launch again. Restarting the instances may migrate them to hardware that has the capacity for all the requested instances. Spread Maximum of 7 running instances per Availability Zone per group. Recommended for applications that have a small number of critical instances that should be kept separate from each other . Spread placement groups provide access to distinct racks and are therefore suitable for mixing instance types or launching instances over time. Cluster Higher per-flow throughput limit of up to 10 Gbps for TCP/IP traffic and are placed in the same high-bisection bandwidth segment of the network. Partition Spreads your instances across logical partitions such that groups of instances in one partition do not share the underlying hardware with groups of instances in different partitions . Used by l arge distributed and replicated workloads .","title":"Placement groups"},{"location":"AWS/Cheatsheet/#autoscaling","text":"Lifecycle hooks enable you to perform custom actions as the Auto Scaling group launches or terminates instances. EC2 Autoscaling Lifecycle Hooks Lifecycle hooks put the instance into a wait state until the script or timeout period ends . With launch templates, you can provision capacity across multiple instance types using both On-Demand Instances and Spot Instances. You can put an instance that is in the InService state into the Standby state, update some software or troubleshoot the instance, and then return the instance to service. Auto Scaling doesn\u2019t terminate an instance that came into service based on EC2 status checks and ELB health checks until the health check grace period expires . Cooldown period: It ensures that the Auto Scaling group does not launch or terminate additional EC2 instances before the previous scaling activity takes effect(default 300s). Amazon EC2 Auto Scaling does not immediately terminate instances with an Impaired status . By default, Amazon EC2 Auto Scaling doesn\u2019t use the results of ELB health checks to determine an instance\u2019s health status when the group\u2019s health check configuration is set to EC2. When there are multiple policies in force at the same time, Auto Scaling chooses the policy that provides the largest capacity for both scale-out and scale-in . The default value for the instance placement tenancy is null, and the instance tenancy is controlled by the tenancy attribute of the VPC. If you set the Launch Configuration Tenancy to default and the VPC Tenancy is set to dedicate, then the instances have dedicated tenancy. If you set the Launch Configuration Tenancy to dedicated and the VPC Tenancy is set to default, then again, the instances have dedicated tenancy. If you have an EC2 Auto Scaling group (ASG) with running instances and you choose to delete the ASG, the instances will be terminated, and the ASG will be deleted . Rebalancing AZs launches new instances before terminating the old ones. Auto Scaling creates a new scaling activity for terminating the unhealthy instance and then terminates it. Later, another scaling activity launches a new instance to replace the terminated instance.","title":"Autoscaling"},{"location":"AWS/Cheatsheet/#s3","text":"S3 standard: There is no minimum storage duration charge and no retrieval fee (use case: if you want to keep data for a few days only) Object-level permissions: For actions inside the bucket (e.g. GetObject), add / after arn, -> arn:aws:s3:::test/ With bucket policies, you can grant users within your AWS Account or other AWS Accounts access to your Amazon S3 resources. The AWS S3 sync command uses the CopyObject APIs to copy objects between S3 buckets. By default, S3 replication only supports copying new Amazon S3 objects after it is enabled. Max upload 5GB per time , for more use multi-part upload. If the object to upload is > 100 MB , you should consider using multipart uploads . Amazon S3 delivers strong read-after-write consistency automatically. You can increase your read or write performance by parallelizing reads with prefixes. Once you version-enable a bucket, it can never return to an unversioned state. Versioning can only be suspended once it has been enabled. No S3 data transfer charges when data is transferred in from the internet. Also, with S3TA, you pay only for transfers that are accelerated . Using the Range HTTP header in a GET Object request, you can fetch a byte range from an object, transferring only the specified portion. A byte-range request is a perfect way to get the beginning of a file . You can place a retention period on an object version. Different versions of a single object can have different retention modes and periods. Max object size 5TB . For replication must enable versioning in source and destination. By default, an S3 object is owned by the AWS account that uploaded it, even in a bucket in a different account. To get full access to the object, the object owner must explicitly grant the bucket owner access. You can create a bucket policy to require external users to grant bucket-owner-full-control when uploading objects so the bucket owner can have full access to the objects. Object lock: store objects as locked(only on versioned buckets). Metadata , which can be included with the object, is not encrypted while being stored on Amazon S3. Therefore, AWS recommends that customers not place sensitive information in Amazon S3 metadata. S3 event notification allows destinations: SQS standard, Lambda, SNS . Allowed names for S3 website endpoints: http://bucket-name.s3-website.Region.amazonaws.com & http://bucket-name.s3-website-Region.amazonaws.com S3 Select scan a subset of an object by specifying a range of bytes to query based on the bucket\u2019s name and the object\u2019s key. With S3 Select, you can use simple structured query language (SQL) statements to filter the contents of an Amazon S3 object and retrieve just the subset of data that you need. CSV, JSON, or Apache Parquet format. S3 can publish notifications for the following events: New object-created events, Object removal events, Restore object events, Reduced Redundancy Storage (RRS) object lost events, Replication events . To encrypt an object at the time of upload , you need to add a header called x-amz-server-side-encryption . To enforce object encryption, create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption header. To enable S3 website: a) An S3 bucket that is configured to host a static website. The bucket must have the same name as your domain or subdomain, b) a registered domain name c) Route 53 as the DNS service for the domain. S3 server access logs provide detailed records for the requests that are made to an S3 bucket. 3,500 requests per second to add data and 5,500 requests per second to retrieve data. You can have an S3 bucket that has different objects stored in S3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, and S3 One Zone-IA. S3 IA S3 One Zone-IA is for data that is accessed less frequently but requires rapid access when needed. The minimum storage duration is 30 days before you can transition objects from S3 Standard to S3 Standard IA or One Zone-IA.(This limitation does not apply to Intelligent Tiering, Glacier, and Glacier Deep Archive) S3 Lifecycle Transitions Supported lifecycle transitions \u2014 waterfall model : The S3 Standard storage class to any other storage class. Any storage class to the S3 Glacier or S3 Glacier Deep Archive storage classes. The S3 Standard-IA storage class to the S3 Intelligent-Tiering or S3 One Zone-IA storage classes. The S3 Intelligent-Tiering storage class to the S3 One Zone-IA storage class. The S3 Glacier storage class to the S3 Glacier Deep Archive storage class. S3 Lifecycle Transitions Encrypted objects remain encrypted throughout the storage class transition process . Glacier Glacier supports encryption by default for both data at rest as well as in transit. The minimal storage duration period is 90 days for the S3 Glacier storage class and 180 days for S3 Glacier Deep Archive . Data can be stored directly in Amazon S3 Glacier Deep Archive. Snowball Snowball Edge storage optimised: 80TB 40 vCPUs, 1 TB of SATA SSD storage, and up to 40 Gb network connectivity. You can\u2019t directly copy data from Snowball Edge devices into AWS Glacier. For data < 10PB or distributed in multiple locations . Snowball Edge compute optimised(52 vCPUs, 42 TB of usable block or object storage, and an optional GPU). Snowball Edge possibility for storage clustering. AWS OpsHub is a graphical user interface you can use to manage your AWS Snowball devices. Snowmobile Each Snowmobile has a total capacity of up to 100 petabytes. For data > 10PB in a single location . Snow Services Comparison","title":"S3"},{"location":"AWS/Cheatsheet/#iam","text":"Permissions Boundary to limit max access of users . They can only be applied to roles or users, not IAM groups. IAM Permissions Boundaries IAM Policy Evaluation Logic: if there is an explicit deny, the final decision is to deny for the resource. When you assume a role, you give up your original permissions and take the permissions of the assigned role. When using a resource-based policy, the principal doesn\u2019t have to give up his permissions. In a policy condition: aws:RequestedRegion represents the target of the API call. You can share an AMI with another account. Trust Policy: only IAM resource-based policy. If you got your certificate from a third-party CA, import the certificate into ACM or upload it to the IAM certificate store. With web identity federation , you don\u2019t need to create custom sign-in code or manage your own user identities. Instead, users of your app can sign in using a well-known external identity provider (IdP), such as Login with Amazon, Facebook, Google, or any other OpenID Connect (OIDC)-compatible IdP. Security Token Service(STS) Temporary security credentials that can control access to your AWS resources. AWS Organizations It does not offer the federation capability. To migrate an account to another Organization: remove member account, send an invite to new Org, Accept the invite to the new Org from the member account. SCPs offer central control over the maximum available permissions for all accounts in your organization, allowing you to ensure your accounts stay within your organization\u2019s access control guidelines. SCPs affect all users and roles in the attached accounts, including the root user. SCPs do not affect any service-linked role.","title":"IAM"},{"location":"AWS/Cheatsheet/#vpc","text":"VPN connection: Virtual Private Gateway endpoint on the AWS VPC side \u2014 Customer Gateway on the on-premises side. You can\u2019t have a VPC with only a public subnet and AWS Site-to-Site VPN. Private IPs allowed ranges: 10.0.0.0/8 (10.0.0.0\u201310.255.255.255), 172.16.0.0/12 (172.16.0.0\u2013172.31.255.255), 192.168.0.0/16(192.168.0.0\u2013192.168.255.255) AWS reserves 5 Ip addresses in each subnet . Shared services VPC, which provides access to services required by workloads in each of the VPCs. This might include directory services or VPC endpoints. Sharing resources from a central location instead of building them in each VPC may reduce administrative overhead and cost. Use AZ ID to uniquely identify the Availability Zones across the two AWS Accounts. By default, non-default subnets have the IPv4 public addressing(assign public IP) attribute set to false , and default subnets have this attribute set to true. You cannot disable IPv4 support for your VPC and subnets since this is the default IP addressing system for Amazon VPC and Amazon EC2. Every subnet that you create is automatically associated with the main route table for the VPC. Allowed block size in VPC is between a /16 netmask (65,536 IP addresses) and /28 netmask. While primary ENIs cannot be detached from an instance, secondary ENIs can be detached and attached to a different instance. Security Groups If nothing is defined in a security group, then all access is blocked. NACL NACLs are stateless, so outbound rules have to be evaluated again. Defined at Subnet level. Should allow outbound traffic from ephemeral ports. NACL rules are evaluated starting with the lowest numbered rule. As soon as a rule matches traffic, it\u2019s applied immediately regardless of any higher-numbered rule that may contradict it. Cloudhub Multiple AWS Site-to-Site VPN connections, you can provide secure communication between sites using the AWS VPN CloudHub, including Direct Connect connections. Supports IP Multicast. Low-cost primary or secondary network connectivity between locations, only for VPNs. Direct Connect Maximum resilience is achieved by separate connections terminating on separate devices in more than one location. Direct Connect High Resiliency Setup Dedicated connection 1\u201310 Gbps. Hosted connection 50Mbps -10Gbps, add or remove capacity on demand. Data in transit is not encrypted , but private. Transit Gateway Network transit hub that you can use to interconnect your virtual private clouds (VPC) and on-premises networks. AWS Transit Gateway also enables you to scale the IPsec VPN throughput with equal-cost multi-path (ECMP) routing support over multiple VPN tunnels. NAT Instance It can be used as a bastion, supports security groups, supports port-forwarding, must disable ec2 flag source/destination check. Nat Gateway Only for IPv4. Set up in a public subnet. In a specific AZ, and can only be used by instances in other subnets. Egress-only Internet Gateway: nat for ipv6.","title":"VPC"},{"location":"AWS/Cheatsheet/#route53","text":"Routing policy multi-value supports up to 8 healthy records for each multi-value query. To integrate an external domain to route53, update the nameservers on the 3rd party registrar with your public hosted zone. To resolve any DNS queries for resources in the AWS VPC from the on-premises network , you can create an inbound endpoint on Route 53 Resolver, and then DNS resolvers on the on-premises network can forward DNS queries to Route 53 Resolver via this endpoint. To resolve DNS queries for any resources in the on-premises network from the AWS VPC , you can create an outbound endpoint on Route 53 Resolver, and then Route 53 Resolver can conditionally forward queries to resolvers on the on-premises network via this endpoint. Cannot create a CNAME record for the top node of the DNS namespace. So, if you register the DNS name mpla.com the zone apex is mpla.com. You can\u2019t create a CNAME record for mpla.com, but you can create an alias record for mpla.com that routes traffic to www.mpla.com. Route 53 doesn\u2019t charge for alias queries to AWS resources, but Route 53 does charge for CNAME queries . For each VPC that you want to associate with the Route 53 hosted zone, change the following VPC settings to true: enableDnsHostnames , enableDnsSupport . You configure active-active failover using any routing policy (or combination of routing policies) other than failover, and you configure active-passive failover using the failover routing policy. Active-Active Failover when you want all of your resources to be available the majority of the time. Active-Passive Failover when you want a primary resource or group of resources to be available the majority of the time, and you want a secondary resource or group of resources to be on standby in case all the primary resources become unavailable.","title":"Route53"},{"location":"AWS/Cheatsheet/#ebs","text":"By default, the root volume for an AMI backed by Amazon EBS is deleted when the instance terminates. For an encrypted EBS volume, data stored at rest on the volume, data moving between the volume and the instance, snapshots created from the volume, and volumes created from those snapshots are all encrypted. GP2: system boot volumes, 1GB \u2014 16TB, max IOPS 16,000; if you add 1TB, you get +3000IOPS for low latency interactive apps. io1/io2: 4GB-16TB, max 64,000 IOPS, 50:1 IOPS:GB ratio. io2 Block Express volumes, Provisioned IOPS (PIOPS) up to 256,000 , with an IOPS:GiB ratio of 1,000:1, f or submillisecond latency for > 64,000 IOPS or 1000 MB/s throughput. Throughput optimised HDD(st1): max throughput 500 MB/s \u2014 max 500 IOPS \u2014 Big data, log processing, data warehouses. Cold HDD(scl): max throughput 250 MB/s \u2014 max 250 IOPS \u2014 Throughput-oriented storage that is infrequently accessed, low storage cost scenarios. Amazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD (io1 or io2) volume to multiple instances with Nitro system that are in the same Availability Zone. Throughput Optimized HDD ( st1 ) and Cold HDD ( sc1 ) volume types cannot be used for boot volumes . EBS Types Comparison Locked to AZ, to attach to other AZ, you have to snapshot it. Copying an unencrypted snapshot allows encryption. When copying an AMI to another region, it automatically creates the underlying EBS snapshot also in the new region. RAID 0 to increase performance. RAID 1 to increase fault tolerance. If the instance is already running, you can set DeleteOnTermination to False using the command line for the root EBS volume. An in-progress snapshot is not affected by ongoing reads and writes to the volume; hence, you can still use the EBS volume normally. Enforce the encryption of the new EBS volumes and snapshot copies that you create with the Encryption by Default feature(no effect on existing EBS volumes or snapshots). If you enable it for a Region, you cannot disable it for individual volumes or snapshots in that Region. When you enable encryption by default, you can launch an instance only if the instance type supports EBS encryption. Amazon EBS does not support asymmetric CMKs. Instance Store Temporary block-level storage for your instance. Ideal for the temporary storage of information that frequently changes, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances, such as a load-balanced pool of web servers Instance Store For high I/O performance, instance store volumes are a better option. You cant resize the instance store.","title":"EBS"},{"location":"AWS/Cheatsheet/#efs","text":"Control which EC2 instances can access your EFS file system with security group rules and IAM policies. 1000s on concurrent NFS clients, 10Gbs throughput. Use EFS Access Points to manage application access. Max I/O performance mode is used to scale to higher levels of aggregate throughput and operations per second \u2014 tradeoff of slightly higher latencies. General Purpose performance mode is ideal for latency-sensitive use cases. POSIX is compliant. Provisioned Throughput mode: for applications with high throughput to storage (MiB/s per TiB) ratios or with requirements greater than those allowed by the Bursting Throughput mode. Bursting Throughput mode: designed to burst to high throughput levels for periods of time. Higher price point than EBS . The maximum days for the EFS lifecycle policy is 90. Amazon FSx for Lustre: Run the world\u2019s most popular high-performance file system. For machine learning, high-performance computing (HPC), video processing, and financial modeling. Ability to both process the \u2018hot data\u2019 in a parallel and distributed fashion as well as easily store the \u2018cold data\u2019 on Amazon S3.","title":"EFS"},{"location":"AWS/Cheatsheet/#rds","text":"Multi A-Z synchronous replication across AZs. Replication between the primary and standby instances does not incur additional data transfer charges. Read replicas asynchronous replication across AZs or cross-region, up to 5. RDS Multi-AZ, Multi-region, Read Replicas Comparison Backups every 5min, ability to restore at any point in time. Supports storage autoscaling. IAM database authentication works with MySQL and PostgreSQL . Use an authentication token with a lifetime of 15 minutes. RDS provides metrics in real-time for the operating system (OS) that your DB instance runs on with Enhanced Monitoring (RDS processes, RDS child processes, OS processes). To encrypt unencrypted RDS database: create a snapshot of your DB instance, and then create an encrypted copy of that snapshot, restore DB from an encrypted snapshot, terminate the previous DB. Upgrades to the database engine level require downtime . Even if your RDS DB instance uses a Multi-AZ deployment, both the primary and standby DB instances are upgraded at the same time. This causes downtime until the upgrade is complete, and the duration of the downtime varies based on the size of your DB instance. RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary, and finally performing maintenance on the old primary, which becomes the new standby. The maximum backup retention period for automated backup is 35 days. Aurora Auto-scales up to 128 TB per database instance. Aurora cluster: one Primary DB instance \u2014 up to 15 replicas(read-only) . Aurora Cluster You can specify the failover priority for Aurora Replicas; each Read Replica is associated with a priority tier (0\u201315). Aurora will promote the Read Replica that has the highest priority (the lowest numbered tier). If two or more Aurora Replicas share the same priority, then Amazon RDS promotes the replica that is the largest in size. Aurora Global Database is designed for globally distributed applications , allowing a single Amazon Aurora database to span multiple AWS regions, sub-second data access in any region . Storage automatically grows in increments of 10GB. In a multi-master cluster, all DB instances can perform write operations(scale writes, avoid downtime for writes) \u2014 continuous availability for applications where you can\u2019t afford even brief downtime for database write operations. Using endpoints, you can map each connection to the appropriate instance or group of instances based on your use case. For clusters with DB instances of different capacities or configurations, you can connect to custom endpoints associated with different subsets of DB instances. Reader endpoint automatically performs load-balancing among all the Aurora Replicas. For diagnosis or tuning, you can connect to a specific instance endpoint to examine details about a specific DB instance. If you are running Aurora Serverless and the DB instance or AZ becomes unavailable, Aurora will automatically recreate the DB instance in a different AZ. If you have a single instance, Aurora will attempt to create a new DB Instance in the same Availability Zone as the original instance. This replacement of the original instance is done on a best-effort basis and may not succeed.","title":"RDS"},{"location":"AWS/Cheatsheet/#dynamodb","text":"DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for Amazon DynamoDB that delivers up to a 10 times performance improvement \u2014 from milliseconds to microseconds . Tables must have provisioned read and write capacity units RCU, WRC. DynamoDB Streams allow changes in DynamoDB to be streamed to other services(read by Lambda etc., 24h retention on streams). Global Tables support multi-region replication, low latency, disaster recovery. Must first enable Streams. Can only query on the primary key, sort key, or indexes. All DynamoDB tables are encrypted . There is no option to enable or disable encryption for new or existing tables. By default, all DynamoDB tables are encrypted under an AWS owned customer master key (CMK), which does not write to CloudTrail logs. If the shard iterator expires immediately before you can use it, this might indicate that the DynamoDB table used by Kinesis does not have enough capacity to store the lease data. To solve, increase the write capacity assigned to the shard table.","title":"DynamoDB"},{"location":"AWS/Cheatsheet/#elasticache","text":"For sub-millisecond latency caching , ElastiCache is the best choice. Memcached Supports multithreaded architecture . Redis Redis HIPAA compliant , supports replication, high availability, and cluster sharding. The in-memory data store that provides sub-millisecond latency . IAM Auth is not supported by ElastiCache. Redis AUTH(enable Redis to require a token (password) before allowing clients to execute commands, thereby improving data security).","title":"Elasticache"},{"location":"AWS/Cheatsheet/#redshift","text":"With Spectrum, you can efficiently query and retrieve structured and semistructured data from files in Amazon S3 without having to load the data into Amazon Redshift tables. Redshift Spectrum For OLAP: online analytical processing. Redshift enhanced VPC routing; copy/unload goes through VPC. Possibility to copy snapshots for a cluster to another region for DR.","title":"Redshift"},{"location":"AWS/Cheatsheet/#cloudwatch","text":"Metrics belong to namespaces; Dimension is an attribute of a metric, Up to 10 dimensions per metric. Automatically recover ec2: If your instance has a public IPv4 address, it retains the public IPv4 address after recovery. During instance recovery, the instance is migrated during an instance reboot, and any data that is in-memory is lost . You can use CloudWatch Events to run Amazon ECS tasks when certain AWS events occur. EventBridge Recommended when you want to build an application that reacts to events from SaaS applications and/or AWS services. Only event-based service that integrates directly with third-party SaaS partners.","title":"Cloudwatch"},{"location":"AWS/Cheatsheet/#encryptionsecrets","text":"Key Policies: control access to keys; you cannot control access without them. Automatic key rotation: CMK every one year. SSE-KMS is a service that combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use server-side encryption with AWS KMS (SSE-KMS), you can specify a customer-managed CMK that you have already created. SSE-KMS provides you with an audit trail that shows when your CMK was used and by whom. Deleting a customer master key (CMK) has enforced a waiting period; you schedule key deletion( minimum of 7 days up to a maximum of 30 days(default) ) SSE-C \u2014 With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys, and Amazon S3 manages the encryption, as it writes to disks and decryption when you access your objects. SSE-S3 \u2014 When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. Uses 256-bit Advanced Encryption Standard (AES-256). Client-side encryption when there is a proprietary encryption algorithm. Secrets Manager Nice RDS integration. Force secret rotation every X days. SSM Parameter Store Allow assigning TTL to a parameter(expiration date) to force update/delete of sensitive data. CloudHSM With dedicated hardware, you manage your own encryption keys. A good option to use with SSE-C. It is possible to lose keys that were created since the most recent daily backup if the CloudHSM cluster that you are using fails and you are not using two or more HSMs.","title":"Encryption/Secrets"},{"location":"AWS/Cheatsheet/#kinesis","text":"Kinesis Data Streams Default data retention 1 day can go up to 7. 1MB/sec/shard ingest capacity. By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream. Use enhanced fan-out if you have multiple consumers retrieving data from a stream in parallel, automatically scales throughput with the number of shards. The ability for multiple apps to consume the same stream concurrently. Ability to consume records in the same order a few hours later . Routing related records to the same record processor. For example, counting and aggregation are simpler when all records for a given key are routed to the same record processor. Kinesis Firehose It automatically scales to match the throughput of your data and requires no ongoing administration. The auto-scaling solution, as there is no need to provision any shards like Kinesis Data Streams. Kinesis Agent cannot write to a Kinesis Firehose for which the delivery stream source is already set as Kinesis Data Streams. Data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, generic HTTP endpoints and Datadog, New Relic, MongoDB, Splunk. Load streaming data into Redshift for near real-time analytics.","title":"Kinesis"},{"location":"AWS/Cheatsheet/#sqs","text":"When you need messaging semantics (ack/fail) and visibility timeout (default 30s). Dynamically increasing concurrency/throughput at read time. FIFO queues support up to 3,000 messages (batch 10 messages per operation- max) per second with batching( 300 without ), have an 80-character queue name limit. Message retention 4 days default, 14 days max . Limit 256kb per message sent. To scale to the same number of consumers as producers , send data with a Group ID attribute . Delay queues let you postpone the delivery of new messages to a queue for several seconds. The default (minimum) delay for a queue is 0 seconds . The maximum is 15 minutes. You can use message timers to set an initial invisibility period for a message added to a queue. Default delay for a message in 0 seconds. The maximum is 15 minutes. Temporary queues help you save development time and deployment costs when using common message patterns such as request-response. To better support short-lived, lightweight messaging destinations, AWS recommends Amazon SQS Temporary Queue Client. The key concept behind the client is the V irtual Queue . Virtual queues let you multiplex many low-traffic queues onto a single SQS queue. SQS Virtual Queues AWS recommends using separate queues to provide prioritization of work . A single SQS message queue can contain an unlimited number of messages. However, there is a 120,000 quota for the number of inflight messages for a standard queue and 20,000 for a FIFO queue. Messages are inflight after they have been received from the queue by a consuming component but have not yet been deleted from the queue. Standard queues provide at least one delivery, which means that each message is delivered at least once. FIFO queues provide exactly-once processing, which means that each message is delivered once and remains available until a consumer processes it and deletes it. Duplicates are not introduced into the queue. An Amazon SQS message can contain up to 10 metadata attributes. By default, ReceiveMessageWaitTimeSeconds is zero, which means it is using Short polling. If it is set to a value greater than zero, then it is Long polling.","title":"SQS"},{"location":"AWS/Cheatsheet/#sns","text":"Event producers send events to 1 topic; we can have many subs. 100000 topics limit Use SNS message filtering to assign a filter policy to the topic subscription, and the subscriber will only receive a message that they are interested in . SNS FIFO for strict message ordering and deduplicated message delivery to one or more subscribers SNS FIFO","title":"SNS"},{"location":"AWS/Cheatsheet/#loadbalancers","text":"LBs can scale but not instantaneously. Elastic Load Balancing stops sending requests to targets that are deregistering. By default, Elastic Load Balancing waits 300s(can be set between 1s to 3600s) seconds before completing the deregistration process, which can help in-flight requests to the target to complete (connection drain). When cross-zone load balancing is enabled , each load balancer node distributes traffic across the registered targets in all enabled Availability Zones evenly . Cross-Zone Load Balancing Enabled By default, cross-zone load balancing is enabled for Application Load Balancer and disabled for Network Load Balancer. ELB cannot distribute incoming traffic for targets deployed in different regions. Access logging is an optional feature of Elastic Load Balancing that is disabled by default. Use to analyze traffic patterns and troubleshoot issues. Application LoadBalancer ALB targets with instance ID route to primary private IP in primary NIC targets using IP addresses route to any private IP from one or more NICs. Host-based Routing : You can route a client request based on the Host field of the HTTP header, allowing you to route to multiple domains from the same load balancer. Path-based Routing : You can route a client request based on the URL path of the HTTP header. HTTP header-based routing : You can route a client request based on the value of any standard or custom HTTP header. HTTP method-based routing : You can route a client request based on any standard or custom HTTP method. Query string parameter-based routing : You can route a client request based on the query string or query parameters. Source IP address CIDR-based routing : You can route a client request based on source IP address CIDR from where the request originates. ALB not registered any targets with the target groups -> 503 error. Use Cognito Authentication via Cognito User Pools for your ALB. With SNI support AWS makes it easy to use more than one certificate with the same ALB. You can host multiple TLS secured applications, each with its own TLS certificate, behind a single ALB. In order to use SNI, all you need to do is bind multiple certificates to the same secure listener on your load balancer. ALB will automatically choose the optimal TLS certificate for each client. ALBs support Weighted Target Groups routing. Network LoadBalancer NLB traffic is routed using the private IP address. Network LB has no security groups, it lets traffic passing by. With Network Load Balancer (NLB), you can offload the decryption/encryption of TLS traffic from your application servers to the NLB. Classic LoadBalancer CLB does not support Server Name Indication (SNI).","title":"LoadBalancers"},{"location":"AWS/Cheatsheet/#lambda","text":"Supports 1000 concurrent executions per AWS account per region , contact support to raise the limit if needed. Supported languages: C#/.NET, GO, node.js, Python, Java, Ruby. lamda@edge: deploy lambda to each region alongside your CloudFront CDN. You can set your memory from 128MB to 10,240MB If your Lambda function accesses a VPC, you must make sure that your VPC has sufficient ENI or subnect IPs capacity to support the scale requirements of your Lambda function. Step Functions Serverless workflows orchestration.","title":"Lambda"},{"location":"AWS/Cheatsheet/#cloudfront","text":"Delivering data out of CloudFront can be more cost-effective than delivering it from S3 directly to your users. Use CloudFront to improve application performance to serve static content from S3. Dynamic content does not flow through regional edge caches but goes directly to the origin \u2014 Proxy methods PUT/POST/PATCH/OPTIONS/DELETE go directly to the origin. Preferred to handle spikes in traffic over GA. You cannot directly integrate Cognito User Pools with CloudFront distribution as you have to create a separate Lambda@Edge function to accomplish the authentication via Cognito User Pools. CloudFront can route to multiple origins based on the content type . Field-level encryption: The sensitive information provided by your users is encrypted at the edge (You can\u2019t encrypt all of the data in a request with field-level encryption; you must specify individual fields to encrypt). CloudFront signed cookies -> provide access to multiple restricted files . CloudFront signed URLs -> access to one file . You can also use an EC2 instance or a custom origin in configuring CloudFront. The Cache-Control and Expires headers control how long objects stay in the cache. The Cache-Control max-age directive lets you specify how long (in seconds) you want an object to remain in the cache before CloudFront gets the object again from the origin server. The minimum expiration time CloudFront supports is 0 seconds for web distributions and 3600 seconds for RTMP distributions.","title":"Cloudfront"},{"location":"AWS/Cheatsheet/#global-acceleratorga","text":"Directs traffic to optimal endpoints over the AWS global network. Improves the availability and performance of your internet applications. Two static anycast IP addresses act as a fixed entry point to your application endpoints. Global Accelerator Overview Good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP. Uses endpoint weights to determine the proportion of traffic that is directed to endpoints in an endpoint group(can be used in blue/green deployments). Global Accelerator Weighted Endpoints WAF Use AWS WAF to block or allow requests based on conditions that you specify, such as the IP addresses. Geographic (Geo) Match Conditions in AWS WAF to restrict application access based on the geographic location of your viewers \u2014 choose the countries from which AWS WAF should allow access. Protects against SQL injection and Cross-Site Scripting . Rate based rules(DDoS protection). Firewall Manager Centrally configure and manage firewall rules across your accounts and applications in AWS Organizations. You can centrally configure AWS WAF rules, AWS Shield Advanced protection, Amazon Virtual Private Cloud (VPC) security groups, AWS Network Firewalls, and Amazon Route 53 Resolver DNS Firewall rules across accounts and resources in your organization. It does not support Network ACLs as of today. AWS Shield DDoS , protection against SYN/UDP floods , reflection attacks, and other layer/3 & layer 4 attacks. EMR Cloud big data platform for processing vast amounts of data using Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto, Hadoop. Beanstalk Easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker. Automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to application health monitoring. You retain full control over the AWS resources powering your application and can access the underlying resources at any time. Application files are stored in S3. The server log files can also optionally be stored in S3 or in CloudWatch Logs. CloudFormation StackSet extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Use the CreationPolicy attribute when you want to wait on resource configuration actions before stack creation proceeds.","title":"Global Accelerator(GA)"},{"location":"AWS/Cheatsheet/#cognito","text":"User pools Provide built-in user management e.g., sign-in and register functionality for apps. Identity pools Provide temporary credentials for AWS access to users. AWS Database Migration Service Seamlessly migrate data from supported sources to relational databases, data warehouses, streaming platforms, and other data stores in the AWS cloud. (e.g., quickly move data from S3 to Kinesis data streams, not only for DBs). Database Migration Service Use Case: Migrate data from S3 to Kinesis","title":"Cognito"},{"location":"AWS/Cheatsheet/#storage-gateway","text":"File Gateway File Gateway Architecture SMB or NFS access to data in S3 with local caching. Volume Gateway Present cloud-based iSCSI block storage volumes to your on-premises applications. Tape Gateway Supports archiving directly to Glacier and Glacier Deep Archive. DataSync Move large data from on-premise to AWS. Can move data directly to Glacier or Glacier Deep Archive . AppSync Store and sync data across mobile and web apps in real-time. CloudTrail By default, CloudTrail event log files are encrypted using Amazon S3 server-side encryption (SSE). Xray AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. End-to-end view of requests as they travel through your application , and shows a map of your application\u2019s underlying components. The X-Ray agent can assume a role to publish data into an account different from the one in which it is running. GuardDuty Threat detection that enables you to continuously monitor and protect your AWS accounts, workloads, and data stored in Amazon S3. Analyses AWS CloudTrail Events, Amazon VPC Flow Logs, and DNS Logs . Disabling the service in the general settings deletes all the remaining data . Macie Discover and protect your sensitive data on Amazon S3. Inspector Helps you check for unintended network accessibility of your Amazon EC2 instances and for vulnerabilities on those EC2 instances. Recognition Automate your image and video analysis with machine learning.","title":"Storage Gateway"},{"location":"AWS/Cheatsheet/#vpc-endpoints","text":"When you create a VPC endpoint, you can attach an endpoint policy that controls access to the service to which you are connecting. Gateway Endpoints GE is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service . S3 & DynamoDB only. Interface Endpoints An elastic network interface with a private IP address from the IP address range of your subnet that serves as an entry point for traffic destined to a supported service.","title":"VPC Endpoints"},{"location":"AWS/Cheatsheet/#enhanced-networking","text":"Elastic Network Adapter(ENA) Enhanced networking capabilities with network speeds of up to 100 Gbps. Supports Windows. Elastic Fabric Adapter (EFA) Network device that you can attach to your Amazon EC2 instance to accelerate High-Performance Computing (HPC) and machine learning applications. ENA with added capabilities. Doesn\u2019t support Windows. EFA support can be enabled either at the launch of the instance or added to a stopped instance. EFA devices cannot be attached to a running instance. API Gateway Rest APIs \u2014 stateful client-server communication. Websocket APIs \u2014 stateless full-duplex communication. All of the APIs created with Amazon API Gateway expose HTTPS endpoints only SWF Simple Workflow Service Use if you need: external signals to intervene , or child processes to return values to parent processes . For decoupled architectures. Provides useful guarantees around task assignments. It ensures that a task is never duplicated and is assigned only once. AWS Backup Centralized backup service. A backup plan is a policy expression that defines when and how you want to back up your AWS resources. AWS Batch Multi-node parallel jobs. AWS ParallelCluster Cluster management tool to deploy HPC, automate creation of vpc, subnet, cluster type etc. AD Connector If you only need to allow your on-premises users to log in to AWS applications and services with their Active Directory credentials. AWS Managed Microsoft AD Configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, using single sign-on (SSO). Data Transfer No charge for inbound data transfer across all services in all Regions. Data transfer from AWS to the internet is charged per service, with rates specific to the originating Region. There is a charge for data transfer across Regions. Data transfer within the same Availability Zone is free. Data transfer over a VPC peering connection that stays within an Availability Zone is free. Data transfer over a VPC peering connection that crosses Availability Zones will incur a data transfer charge for ingress/egress traffic. If the VPCs are peered across Regions, standard inter-Region data transfer charges will apply. Data processing charges apply for each GB sent from a VPC, Direct Connect, or VPN to Transit Gateway. Direct Connect & VPN also incur charges for data flowing out of AWS. Important ports: FTP: 21 SSH: 22 SFTP: 22 (same as SSH) HTTP: 80 HTTPS: 443 RDP: TCP 3389 and UDP 3389 RDS Databases ports: PostgreSQL: 5432 MySQL: 3306 Oracle RDS: 1521 MSSQL Server: 1433 MariaDB: 3306 (same as MySQL) Aurora: 5432 (if PostgreSQL compatible) or 3306 (if MySQL compatible)","title":"Enhanced Networking"},{"location":"AWS/Cheatsheet/#disaster-recovery-in-aws","text":"RPO: Recovery Point Objective -> how much data loss we are willing to recover RTO: Recovery Time Objective -> downtime between disaster and RTO","title":"Disaster Recovery in AWS"},{"location":"Books/The%20Effective%20Engineer/","text":"Effective Engineer - Notes By Edmond Lau Highly Recommended http://www.theeffectiveengineer.com/ What's an Effective Engineer? They are the people who get things done. Effective Engineers produce results. Adopt the Right Mindsets Focus on High Leverage Activities Leverage = Impact Produced / Time Invested Use Leverage as Your Yardstick for Effectiveness 80% of the impact comes from 20% of the work. Focus on high leverage and not just easy wins. Optimize for Learning Change jobs if you have to. Optimizing for learning is high leverage. Adopt a growth mindset. Talk to people. Become good at telling stories. It gets better with time. Those with a growth mindset believe that they can cultivate and grow their intelligence and skills through effort. Own your story. Invest in the rate of learning Learning compounds. Compounding leads to exponential growth. Earlier the compounding starts, the better. Working on unchallenging tasks is a huge opportunity cost. You missed out on compounded learning. Prioritize learning over profitability. Invest your time in activities with the highest learning rate. Seek Work Environments Conducive to Learning Fast Growth: Companies where #problems >> #resources. Opportunity to choose high impact work. Make sure you are working on high priority projects. Openness: Look for culture with curiosity, where everyone is encouraged to ask questions. Fast Paced. People smarter than you. Autonomy: Freedom to choose what to work on. Smaller companies => More autonomy. While on Job Make a daily habit of acquiring new skills. Read code written by brilliant engineers. Jump fearlessly into code you don't know. Always be learning. Invest in skills that are in high demand. Read Books. Attend Conferences. Build and maintain strong relationships. Prioritize Regularly Opportunity cost of working on wrong ideas can set back growth by years. Prioritize tasks based on ROI. Regular prioritization is high leverage activity. On TODO Lists: Maintain a 'single' todo lists where all tasks are listed. Don't try to remember stuff. Brain is bad at remembering. It's rather good at processing. Ask yourself regularly: Is this the most important thing I should be working on? Focus on what directly produces value. Learn to say no. Focus on the important and non-urgent. Find ways to get into flow. \u201cA state of effortless concentration so deep that they lose their sense of time, of themselves, of their problems.\u201d When possible, preserve larger blocks of focused time in your schedule. Limit the amount of Work in Progress. Cost of context switching is high. Prioritizing is difficult. Prioritization is high leverage. It has huge impact on your ability to get right things done. Invest in Iteration Speed Continuous Deployment is high leverage. Will save a lot of time in manual deployment of code. They are the people who get things done. Effective Engineers produce results. Move fast to learn fast. Move fast and break things. Moving fast enables us to build more things and learn at faster rate. Invest in time saving tools. If you have to do something more than twice, write a tool the third time. Tools are multipliers that allow your to scale your impact beyond the confines of a day. Faster tools get used more often. Faster tools can enable new workflows that previously weren't possible. Productivity skyrockets with tools. Time saving property of tools also scale with team adoption. Shorten your debugging and validation Loops. Extra time spent in optimizing debugging workflow can help you fix annoying bugs with less headache. Debugging is hard. It's time consuming. Upfront investments to shorten debugging loops are worth it. High test coverage to reduce build and site breakages. Fast unit tests to encourage people to run them. Fast and incremental compiles and reloads to reduce development time. Master you programming environment. One editor. One high level language. Shell. Keyboard > Mouse. Automate manual workflows. Use interactive shell. Make running specific tests easy. Faster you can iterate, faster you can learn. Measure what you want to Improve Use metric to drive progress. If you can't measure it, you can't improve it. Good metric. Helps you focus on right things. Drives forward progress. Helps you guard against future regressions. Performance ratcheting : Any change should strictly improve the metric. Bad metric can lead to unwanted behavior. Examples: hours worked < productivity. click through rates < long click through rates. Metric you choose influences your decisions and behavior. Look for metric that, when optimized, maximizes impact for the team. Actionable metric - Whose movement can be casually explained by team's effort. Responsive metric - Updates quickly to give back feedback whether a given change was =ve or -ive. Choosing a metric is high leverage. Dedicate time to pick right metric. Instrument everything to understand what's going on. Measure anything, measure everything. Graphite, statsd. A single line of code lets you define a new counter or timer on the fly. Measuring goals you want to achieve is high leverage. Internalize useful numbers. Knowledge of useful numbers provide a valuable shortcut for knowing where to invest efforts to maximize gains. Need upfront work. Need not be accurate, ballpark idea suffices. Knowing useful numbers enables you to do back of the envelope calculations to quickly estimate the performance properties of a design without actually building it. Internalizing useful number help you spot anomalies. Be skeptical about data integrity. Log data liberally. Build tools to iterate on data accuracy sooner. Examine data sooner. When numbers look off, dig in to it sooner. Measure your progress. Carefully choose your top-level metric. Instrument your system. Know your numbers. Prioritize data integrity. Validate your ideas early and often. Not validating early leads to wasted efforts. Don't delay get feedback. Find low effort ways to validate work. Power of small batches. Helps you avoid making a big mistake by stopping the flow. Approach problem iteratively. No large implementations. Working solo? Be wary. Be extra vocal and get feedback. Improve project estimation skills. Beware of mythical man month. Communication overhead is significant. Reduce risk early. Rewrite projects - almost always fail. Additional hours hurt productivity. Causes burnout. Do the riskiest task first. Allow buffer room for the unknown. Balance Quality with Pragmatism High code quality. Code readability. Establish sustainable code review process. Code reviews help: Catch bugs and design problems early. Sharing working knowledge of the codebase. Increases long term agility. Easier to understand, quicker to modify. Manage complexity through Abstraction Example: MapReduce. Right abstractions make huge difference. \u201cPick the right ones, and programming will flow naturally from design; modules will have small and simple interfaces; and new functionality will more likely fit in without extensive reorganization,\u201d \u201cPick the wrong ones, and programming will be a series of nasty surprises: interfaces will become baroque and clumsy as they are forced to accommodate unanticipated interactions, and even the simplest of changes will be hard to make.\u201d The right abstraction can increase engineering productivity by an order of magnitude. Simple abstractions avoid interweaving multiple concepts, so that you can reason about them independently rather than being forced to consider them together. Designing good abstractions take work. An abstraction's usage and popularity provides a reasonable proxy for its quality. Automate Testing Unit test cases and some integration testing provide a scalable way of managing growing codebase. A suite of extensive and automated tests can reduce overall error rates by validating the quality and by safeguarding against regressions. Tests also allow engineers to make changes, especially large refactorings, with significantly higher confidence. Despite its benefits, it can be difficult to foster a culture of automated testing. Focus on high leverage tests. Writing more tests, creating a virtuous feedback cycle and saving more development time. Repay Technical Debt Technical debt refers to all the deferred work that\u2019s necessary to improve the health and quality of the codebase and that would slow us down if left unaddressed. Accumulating technical debt is fine as far as it is repaid within time. Refactor often. Reduce Operational Complexity Keep no. of technologies low. Don\u2019t sway towards shiny new technologies. Every additional technology you add is is guaranteed to go wrong eventually. Will need your time. Do the simple thing first. Embrace operational simplicity. The first solution that comes to mind is generally complex. Don't stop. Keep peeling off the layers of onion. Simplify the architecture to reduce their operational burden. \u201cWhat\u2019s the simplest solution that can get the job done while also reducing our future operational burden?\u201d Discipline to focus on simplicity is high leverage. Fail Fast Fail immediately and visibly. Doesn\u2019t necessarily mean crashing your programs for users. fail-fast to surface issues immediately. Failing fast is high leverage as it saves debugging time. Relentlessly Automate Automating mechanics is good. Automating decision making - no. Hone your ability to respond and recover quickly. Leverage recovering quickly > Leverage preventing failures. \u201cscript for success,\u201d practice failure scenarios, and work on our ability to recover quickly. Make batch process idempotent Make processes retryable, i.e., not leaving any global state. Invest in your team's Growth Invest in onboarding. The higher you climb up the engineering ladder, the more your effectiveness will be measured not by your individual contributions but by your impact on the people around you. \"You\u2019re a staff engineer if you\u2019re making a whole team better than it would be otherwise. You\u2019re a principal engineer if you\u2019re making the whole company better than it would be otherwise. And you\u2019re distinguished if you\u2019re improving the industry.\u201d \ufffc- Focus primarily on making everyone around you succeed. Your career depends on your team's success. Make hiring everyone's responsibility. Shared ownership of code. Keep bus factor more than one. Shared ownership removes isolated silos of information. Build collective wisdom through post mortems. Invest in automated testing. Automated test cases lead to higher confidence when refactoring. Write test cases when the code is fresh in mind. Don\u2019t be dogmatic about 100% code coverage. Value of tests increases over time and cost to write goes down. Hire the best. Surround yourself with great advisors \u2600\ufe0f : \u201cLeverage is the lens through which effective engineers view their activities. \u201d \u2600\ufe0f 10 Books to read: Peopleware Productive projects and Teams. Amazon. My Summary. Team Geek: A Software Developer\u2019s Guide to Working Well with Others. (Debugging Teams) Amazon. My Summary. High Output Management Getting Things Done: The Art of Stress-Free Productivity The 4-Hour Workweek: Escape 9-5, Live Anywhere, and Join the New Rich The 7 Habits of Highly Effective People: Powerful Lessons in Personal Change Conscious Business: How to Build Value Through Values Your Brain at Work Flow: The Psychology of Optimal Experience Succeed: How We Can Reach Our Goals Blogs: Recommended Blogs To Follow: http://www.theeffectiveengineer.com/ - The Effective Engineer is my personal blog, where I write about engineering habits, productivity tips, leadership, and culture. http://www.kalzumeus.com/ - Patrick McKenzie runs his own software business and has written many excellent long-form articles on career advice, consulting, SEO, and software sales. http://katemats.com/ - Kate Matsudaira, who has worked at large companies like Microsoft and Amazon as well as at startups, shares advice about tech, leadership, and life on her blog. http://randsinrepose.com/ - Michael Lopp has worked for many years in leadership positions at Netscape, Apple, Palantir, and Pinterest, and writes about tech life and engineering management. http://softwareleadweekly.com/ - Oren Ellenbogen curates a high-quality weekly newsletter on engineering leadership and culture. http://calnewport.com/ - Cal Newport, an assistant professor of computer science at Georgetown, focuses on evidence-based advice for building a successful and fulfilling life. http://www.joelonsoftware.com/ - Joel Spolsky, the co-founder of Stack Exchange, provides all sorts of programming pearls of wisdom on his blog. http://martinfowler.com/ - Martin Fowler, author of the book Refactoring, writes about how to maximize the productivity of software teams and provides detailed write-ups of common programming patterns. http://pgbovine.net/ - Philip Guo, a computer science professor, has written extensively and openly about his graduate school and work experiences.","title":"Effective Engineer - Notes"},{"location":"Books/The%20Effective%20Engineer/#effective-engineer-notes","text":"By Edmond Lau Highly Recommended http://www.theeffectiveengineer.com/","title":"Effective Engineer - Notes"},{"location":"Books/The%20Effective%20Engineer/#whats-an-effective-engineer","text":"They are the people who get things done. Effective Engineers produce results.","title":"What's an Effective Engineer?"},{"location":"Books/The%20Effective%20Engineer/#adopt-the-right-mindsets","text":"","title":"Adopt the Right Mindsets"},{"location":"Books/The%20Effective%20Engineer/#focus-on-high-leverage-activities","text":"Leverage = Impact Produced / Time Invested Use Leverage as Your Yardstick for Effectiveness 80% of the impact comes from 20% of the work. Focus on high leverage and not just easy wins.","title":"Focus on High Leverage Activities"},{"location":"Books/The%20Effective%20Engineer/#optimize-for-learning","text":"Change jobs if you have to. Optimizing for learning is high leverage. Adopt a growth mindset. Talk to people. Become good at telling stories. It gets better with time. Those with a growth mindset believe that they can cultivate and grow their intelligence and skills through effort. Own your story. Invest in the rate of learning Learning compounds. Compounding leads to exponential growth. Earlier the compounding starts, the better. Working on unchallenging tasks is a huge opportunity cost. You missed out on compounded learning. Prioritize learning over profitability. Invest your time in activities with the highest learning rate. Seek Work Environments Conducive to Learning Fast Growth: Companies where #problems >> #resources. Opportunity to choose high impact work. Make sure you are working on high priority projects. Openness: Look for culture with curiosity, where everyone is encouraged to ask questions. Fast Paced. People smarter than you. Autonomy: Freedom to choose what to work on. Smaller companies => More autonomy. While on Job Make a daily habit of acquiring new skills. Read code written by brilliant engineers. Jump fearlessly into code you don't know. Always be learning. Invest in skills that are in high demand. Read Books. Attend Conferences. Build and maintain strong relationships.","title":"Optimize for Learning"},{"location":"Books/The%20Effective%20Engineer/#prioritize-regularly","text":"Opportunity cost of working on wrong ideas can set back growth by years. Prioritize tasks based on ROI. Regular prioritization is high leverage activity. On TODO Lists: Maintain a 'single' todo lists where all tasks are listed. Don't try to remember stuff. Brain is bad at remembering. It's rather good at processing. Ask yourself regularly: Is this the most important thing I should be working on? Focus on what directly produces value. Learn to say no. Focus on the important and non-urgent. Find ways to get into flow. \u201cA state of effortless concentration so deep that they lose their sense of time, of themselves, of their problems.\u201d When possible, preserve larger blocks of focused time in your schedule. Limit the amount of Work in Progress. Cost of context switching is high. Prioritizing is difficult. Prioritization is high leverage. It has huge impact on your ability to get right things done.","title":"Prioritize Regularly"},{"location":"Books/The%20Effective%20Engineer/#invest-in-iteration-speed","text":"Continuous Deployment is high leverage. Will save a lot of time in manual deployment of code. They are the people who get things done. Effective Engineers produce results. Move fast to learn fast. Move fast and break things. Moving fast enables us to build more things and learn at faster rate. Invest in time saving tools. If you have to do something more than twice, write a tool the third time. Tools are multipliers that allow your to scale your impact beyond the confines of a day. Faster tools get used more often. Faster tools can enable new workflows that previously weren't possible. Productivity skyrockets with tools. Time saving property of tools also scale with team adoption. Shorten your debugging and validation Loops. Extra time spent in optimizing debugging workflow can help you fix annoying bugs with less headache. Debugging is hard. It's time consuming. Upfront investments to shorten debugging loops are worth it. High test coverage to reduce build and site breakages. Fast unit tests to encourage people to run them. Fast and incremental compiles and reloads to reduce development time. Master you programming environment. One editor. One high level language. Shell. Keyboard > Mouse. Automate manual workflows. Use interactive shell. Make running specific tests easy. Faster you can iterate, faster you can learn.","title":"Invest in Iteration Speed"},{"location":"Books/The%20Effective%20Engineer/#measure-what-you-want-to-improve","text":"Use metric to drive progress. If you can't measure it, you can't improve it. Good metric. Helps you focus on right things. Drives forward progress. Helps you guard against future regressions. Performance ratcheting : Any change should strictly improve the metric. Bad metric can lead to unwanted behavior. Examples:","title":"Measure what you want to Improve"},{"location":"Books/The%20Effective%20Engineer/#hours-worked-productivity","text":"click through rates < long click through rates. Metric you choose influences your decisions and behavior. Look for metric that, when optimized, maximizes impact for the team. Actionable metric - Whose movement can be casually explained by team's effort. Responsive metric - Updates quickly to give back feedback whether a given change was =ve or -ive. Choosing a metric is high leverage. Dedicate time to pick right metric. Instrument everything to understand what's going on. Measure anything, measure everything. Graphite, statsd. A single line of code lets you define a new counter or timer on the fly. Measuring goals you want to achieve is high leverage. Internalize useful numbers. Knowledge of useful numbers provide a valuable shortcut for knowing where to invest efforts to maximize gains. Need upfront work. Need not be accurate, ballpark idea suffices. Knowing useful numbers enables you to do back of the envelope calculations to quickly estimate the performance properties of a design without actually building it. Internalizing useful number help you spot anomalies. Be skeptical about data integrity. Log data liberally. Build tools to iterate on data accuracy sooner. Examine data sooner. When numbers look off, dig in to it sooner. Measure your progress. Carefully choose your top-level metric. Instrument your system. Know your numbers. Prioritize data integrity.","title":"hours worked &lt; productivity."},{"location":"Books/The%20Effective%20Engineer/#validate-your-ideas-early-and-often","text":"Not validating early leads to wasted efforts. Don't delay get feedback. Find low effort ways to validate work. Power of small batches. Helps you avoid making a big mistake by stopping the flow. Approach problem iteratively. No large implementations. Working solo? Be wary. Be extra vocal and get feedback.","title":"Validate your ideas early and often."},{"location":"Books/The%20Effective%20Engineer/#improve-project-estimation-skills","text":"Beware of mythical man month. Communication overhead is significant. Reduce risk early. Rewrite projects - almost always fail. Additional hours hurt productivity. Causes burnout. Do the riskiest task first. Allow buffer room for the unknown.","title":"Improve project estimation skills."},{"location":"Books/The%20Effective%20Engineer/#balance-quality-with-pragmatism","text":"High code quality. Code readability. Establish sustainable code review process. Code reviews help: Catch bugs and design problems early. Sharing working knowledge of the codebase. Increases long term agility. Easier to understand, quicker to modify.","title":"Balance Quality with Pragmatism"},{"location":"Books/The%20Effective%20Engineer/#manage-complexity-through-abstraction","text":"Example: MapReduce. Right abstractions make huge difference. \u201cPick the right ones, and programming will flow naturally from design; modules will have small and simple interfaces; and new functionality will more likely fit in without extensive reorganization,\u201d \u201cPick the wrong ones, and programming will be a series of nasty surprises: interfaces will become baroque and clumsy as they are forced to accommodate unanticipated interactions, and even the simplest of changes will be hard to make.\u201d The right abstraction can increase engineering productivity by an order of magnitude. Simple abstractions avoid interweaving multiple concepts, so that you can reason about them independently rather than being forced to consider them together. Designing good abstractions take work. An abstraction's usage and popularity provides a reasonable proxy for its quality.","title":"Manage complexity through Abstraction"},{"location":"Books/The%20Effective%20Engineer/#automate-testing","text":"Unit test cases and some integration testing provide a scalable way of managing growing codebase. A suite of extensive and automated tests can reduce overall error rates by validating the quality and by safeguarding against regressions. Tests also allow engineers to make changes, especially large refactorings, with significantly higher confidence. Despite its benefits, it can be difficult to foster a culture of automated testing. Focus on high leverage tests. Writing more tests, creating a virtuous feedback cycle and saving more development time.","title":"Automate Testing"},{"location":"Books/The%20Effective%20Engineer/#repay-technical-debt","text":"Technical debt refers to all the deferred work that\u2019s necessary to improve the health and quality of the codebase and that would slow us down if left unaddressed. Accumulating technical debt is fine as far as it is repaid within time. Refactor often.","title":"Repay Technical Debt"},{"location":"Books/The%20Effective%20Engineer/#reduce-operational-complexity","text":"Keep no. of technologies low. Don\u2019t sway towards shiny new technologies. Every additional technology you add is is guaranteed to go wrong eventually. Will need your time. Do the simple thing first. Embrace operational simplicity. The first solution that comes to mind is generally complex. Don't stop. Keep peeling off the layers of onion. Simplify the architecture to reduce their operational burden. \u201cWhat\u2019s the simplest solution that can get the job done while also reducing our future operational burden?\u201d Discipline to focus on simplicity is high leverage.","title":"Reduce Operational Complexity"},{"location":"Books/The%20Effective%20Engineer/#fail-fast","text":"Fail immediately and visibly. Doesn\u2019t necessarily mean crashing your programs for users. fail-fast to surface issues immediately. Failing fast is high leverage as it saves debugging time.","title":"Fail Fast"},{"location":"Books/The%20Effective%20Engineer/#relentlessly-automate","text":"Automating mechanics is good. Automating decision making - no. Hone your ability to respond and recover quickly. Leverage recovering quickly > Leverage preventing failures. \u201cscript for success,\u201d practice failure scenarios, and work on our ability to recover quickly. Make batch process idempotent Make processes retryable, i.e., not leaving any global state.","title":"Relentlessly Automate"},{"location":"Books/The%20Effective%20Engineer/#invest-in-your-teams-growth","text":"Invest in onboarding. The higher you climb up the engineering ladder, the more your effectiveness will be measured not by your individual contributions but by your impact on the people around you. \"You\u2019re a staff engineer if you\u2019re making a whole team better than it would be otherwise. You\u2019re a principal engineer if you\u2019re making the whole company better than it would be otherwise. And you\u2019re distinguished if you\u2019re improving the industry.\u201d \ufffc- Focus primarily on making everyone around you succeed. Your career depends on your team's success. Make hiring everyone's responsibility. Shared ownership of code. Keep bus factor more than one. Shared ownership removes isolated silos of information. Build collective wisdom through post mortems. Invest in automated testing. Automated test cases lead to higher confidence when refactoring. Write test cases when the code is fresh in mind. Don\u2019t be dogmatic about 100% code coverage. Value of tests increases over time and cost to write goes down. Hire the best. Surround yourself with great advisors \u2600\ufe0f : \u201cLeverage is the lens through which effective engineers view their activities. \u201d \u2600\ufe0f","title":"Invest in your team's Growth"},{"location":"Books/The%20Effective%20Engineer/#10-books-to-read","text":"Peopleware Productive projects and Teams. Amazon. My Summary. Team Geek: A Software Developer\u2019s Guide to Working Well with Others. (Debugging Teams) Amazon. My Summary. High Output Management Getting Things Done: The Art of Stress-Free Productivity The 4-Hour Workweek: Escape 9-5, Live Anywhere, and Join the New Rich The 7 Habits of Highly Effective People: Powerful Lessons in Personal Change Conscious Business: How to Build Value Through Values Your Brain at Work Flow: The Psychology of Optimal Experience Succeed: How We Can Reach Our Goals","title":"10 Books to read:"},{"location":"Books/The%20Effective%20Engineer/#blogs","text":"Recommended Blogs To Follow: http://www.theeffectiveengineer.com/ - The Effective Engineer is my personal blog, where I write about engineering habits, productivity tips, leadership, and culture. http://www.kalzumeus.com/ - Patrick McKenzie runs his own software business and has written many excellent long-form articles on career advice, consulting, SEO, and software sales. http://katemats.com/ - Kate Matsudaira, who has worked at large companies like Microsoft and Amazon as well as at startups, shares advice about tech, leadership, and life on her blog. http://randsinrepose.com/ - Michael Lopp has worked for many years in leadership positions at Netscape, Apple, Palantir, and Pinterest, and writes about tech life and engineering management. http://softwareleadweekly.com/ - Oren Ellenbogen curates a high-quality weekly newsletter on engineering leadership and culture. http://calnewport.com/ - Cal Newport, an assistant professor of computer science at Georgetown, focuses on evidence-based advice for building a successful and fulfilling life. http://www.joelonsoftware.com/ - Joel Spolsky, the co-founder of Stack Exchange, provides all sorts of programming pearls of wisdom on his blog. http://martinfowler.com/ - Martin Fowler, author of the book Refactoring, writes about how to maximize the productivity of software teams and provides detailed write-ups of common programming patterns. http://pgbovine.net/ - Philip Guo, a computer science professor, has written extensively and openly about his graduate school and work experiences.","title":"Blogs:"},{"location":"Books/Your%20Money%20Ratios/","text":"","title":"Your Money Ratios"},{"location":"Books/Your%20Money%20or%20Your%20Life/","text":"Step 1: Make Peace with your Past Add up all the money you\u2019ve earned in your life, then add up your net worth today. How much have you managed to hold onto? How much did you spend? For most people, this yields an unpleasant surprise.. but it\u2019s okay, for there is no sense beating yourself up over past mistakes. Step 2: Figure out your Real Earnings and Spending The idea here is that your real hourly wage is much lower than you think. You can figure it out as follows, and I\u2019ll even put in some plausible figures for a person with a $50,000 annual salary: Take your total monthly income after federal and state taxes: ($3500) Then subtract all work-related expenses (commuting, clothes, restaurant lunches, housekeepers, daycare,de-stressing activities etc) ($1500) Divide this by your total work time (including commuting, dressing up, clothes cleaning, unwinding time, etc.) (248 hours) The net result is that you take home a lot less than you think, and spend a lot more time doing it. In the example above, the $50k earner ends up bringing home only $8.06 for each hour spent in activities directly related to the job. Thus, when you decide to buy yourself an 8 dollar treat at Starbucks or at the pub, you\u2019ve really just burned off an entire hour of \u201clife energy\u201d which you\u2019ll never get back \u2013 you have to add that hour to the end of your work career to achieve financial independence. Tracking your spending is the easy part \u2013 the book recommends you use a notebook to handle everything, whereas I just do all of my spending by credit card, allowing it to be tracked automatically. The key, however, is you should know exactly what you buy each day, and why you decide to buy it. No more unconscious impulse shopping. 3: Create Monthly Reports for Yourself Keep a table of all income and all spending for each month, break it into categories, and convert the figures into \u201chours of life energy spent\u201d. Restaurant meals: 20 hours., etc. I find that the \u201cMint\u201d financial tool does an acceptable job of this for me, but the book recommends you do it in more detail. 4: Three Questions that will Supposedly Transform your Life: For each of the categories above, ask yourself: - Did I receive fulfillment in proportion to the hours of life energy spent? - Is this expenditure in alignment with my goals and life purpose? - How might this expenditure change if I didn\u2019t have to work for a living? (more, less, same) 5: Keep a prominent (i.e. right on your kitchen wall) graph of income and expenses You keep doing this for multiple months which will grow into multiple years. The authors report that most people start to see their income grow even as their expenses shrink, since they are now learning to spend more consciously. Although I don\u2019t have anything on my kitchen wall, we do maintain a history of spreadsheet versions and graphs of savings that dates back several years. But if you are a beginner who still wrangles with optional luxury purchases while still in debt, the kitchen wall is a good idea. 6: Learn to Value your Life Energy by Minimizing Spending This is the meat of anyone\u2019s financial independence \u2013 learning to spend your money efficiently on the things you do get true fulfillment from, and not spend it all on the things you don\u2019t. The book presents 101 tips, most of which have been covered here on this blog at various times. 7: Maximize your Earnings Adopt a positive attitude about your work and appreciate the earnings as a tool which gets you to financial independence.. rather than feeling like a victim of outside forces like the economy or a recession. Seek to earn more, and don\u2019t be limited to work only in your current field \u2013 after all, you\u2019ll be retiring soon anyway, meaning every activity will soon be open to you whether paid or unpaid. 8: Watch for the Crossover Point This is when your passive income from investments equals your expenses. When you reach that point \u2013 DingDing! \u2013 you are Financially Independent. However, the authors define this as \u201cMonthly Income = Capital x Current long-term interest rate/12 months\u201d, since they like government bonds as their retirement income vehicle, which currently pay approximately zero after adjusting for inflation. But Mustachians of course have other options, discussed below. 9: Managing your Money \u201cBecome knowledgeable and invest your capital in such a way as to provide an absolutely safe income sufficient to meet your basic needs for life\u201d Here\u2019s where one of the most significant differences pops up between YMOYL and MMM (and other modern takes on financial independence). In the early 1980s, you could buy 30-year government bonds with a nominal yield of over 12%. Even in the surrounding time periods, yields were well over 7%. The YM authors liked the guaranteed return and decided to use these bonds as a complete income source*. Due to our continued hangover from the financial crisis, the latest figure for the 30-year bonds is about 2.9%. While it is still possible to retire on bonds, I consider an over-50% reduction in investment returns in exchange for \u201csafety\u201d to be too high a price to pay. Safety is just an expensive illusion anyway. So instead of bonds, we focus here on stocks, dividends, owning rental real estate (or its passive cousin REITs ), and even a bit of wacky new higher risk/return stuff like peer-to-peer lending . And on top of that, I don\u2019t consider \u201cretirement\u201d to mean \u201cnever accepting money for things you do\u201d, so I allow you to do fun things that happen to generate money in retirement as well. Your Money or Your Life is a wise book, and the authors were clearly motivated by what they saw was a pointless death march of society. Workworkwork, Buybuybuy, TrashDestroyWaste, Die. Even 20 years ago, when the first clunky SUVs were coming to market and trailblazing a path to widespread stupidity, this pattern was already obvious. And Joe and Vicki were wise to it, trying to guide society away from its wasteful ways and vividly aware that our consumption is an ongoing trainwreck of environmental destruction. The bad news is that we went through some pretty shitty decades since then, when measured by the spread of the very consumer disease the book was fighting against. Cars turned into personal trucks, commutes grew, suburbs sprawled, and China joined the party, building a communist copy of the Great American Smokestack, flooding their own country with asphalt and ours with cheap manufactured goods. Americans kept working more so they could borrow more and buy more, we grew much fatter and less happy, and generally continue to live our lives in the most blind and inefficient way possible on average. The good news is, the Internet happened. Of course, it spawned an acceleration of technological progress, giving us things like remote working and energy-efficient products. But technology can\u2019t save the world by itself \u2013 in the wrong hands, it just allows us to consume more efficiently, which means consuming more. It\u2019s a good tool, but it\u2019s not enough. The good news comes from the free exchange of ideas. Only now can the ideas of the non-wealthy majority compete equally with the billion-dollar budgets of crusty old companies seeking to prolong over-consumption. Nowadays, even an untrained individual can sit on the couch and type some shit into the computer, and it can reach a wider audience than a successful book might have in the past. So imagine what a big group of people could accomplish, some of them with influence over companies and governments, if they all started grooving on the right message.","title":"Your Money or Your Life"},{"location":"Coding/Setting%20Up%20Mac%20for%20Coding/","text":"Mac Setup Check you're admin Spotlight >> Users & Groups >> Password = Check Allow user to admin this computer Run following on Terminal sudo plutil -p /var/db/dslocal/nodes/Default/users/root.plist | grep -A 2 passwd Below output means root user is not enabled. ```sh \"passwd\" => [ 0 => \"*\" ] ``` Below output means root user is enabled. sh \"passwd\" => [ 0 => \"********\" ] To enable root user, Spotlight >> Directory Utility = Click on the lock at the bottom to unlock it. Edit menu >> Enable Root User Customize Mac Finder - Show Finder path in Status bar: View \u00bb Show Path Bar - Drag and drop your favourite folders into the left-side Favorites bar e.g. Macintosh HD, Repos. - Show hidden files and folders using Terminal commands: ```sh defaults write com.apple.finder AppleShowAllFiles YES Killall Finder // Then, restart the Finder by holding down Option+Control and clicking the Finder icon in the Dock, then choose Relaunch ``` Trackpad Spotlight >> Trackpad >> Point & Click = Check Tap to Click Spotlight >> Trackpad >> Point & Click = Un-check Force Click and Haptic Feedback Spotlight >> Trackpad >> Point & Click = Increase Tracking Speed Keyboard Spotlight >> Keyboard >> Keyboard = Check Use F1,F2 keys as standard keys Spotlight >> Keyboard >> Shortcuts = Double click Show Desktop and hit Cmd+D Desktop Spotlight >> Desktop & Screensavers >> Screen Saver = Check 'Show with clock' Spotlight >> Desktop & Screensavers >> Screen Saver => Hot Corners Left Bottom = Desktop Right Bottom = Screen Saver Locking - Spotlight >> Keychain Access >> Preferences = Check 'Show keychain status in menu bar'. Screen Saver - From Finder >> Go To >> /System/Library/Frameworks/ScreenSaver.framework/Versions/A/Resources/ (Sierra) - Drag and Drop ScreenSaveEngine app on Dock. - High Sierra = /System/Library/CoreServices/ Development Tools & Settings VS Code iTerm2 Sublime SnagIt Spectacle Date-O Mac Tips Start-up Programs Spotlight >> Users & Groups >> Login Items = Add apps Productivity Tips Command Purpose CMD+Ctrl+F Full Screen Spotlight Find app, use as calculator, dictionary File delete Backspace delete Back Delete del Forward delete fn del Capture Screen CMD+SHIFT+4 Move Up/Down the Folders CMD + Up/Down CMD Full CMD Full","title":"Setting Up Mac for Coding"},{"location":"Coding/Setting%20Up%20Mac%20for%20Coding/#mac-setup","text":"Check you're admin Spotlight >> Users & Groups >> Password = Check Allow user to admin this computer Run following on Terminal sudo plutil -p /var/db/dslocal/nodes/Default/users/root.plist | grep -A 2 passwd Below output means root user is not enabled. ```sh \"passwd\" => [ 0 => \"*\" ] ``` Below output means root user is enabled. sh \"passwd\" => [ 0 => \"********\" ] To enable root user, Spotlight >> Directory Utility = Click on the lock at the bottom to unlock it. Edit menu >> Enable Root User Customize Mac Finder - Show Finder path in Status bar: View \u00bb Show Path Bar - Drag and drop your favourite folders into the left-side Favorites bar e.g. Macintosh HD, Repos. - Show hidden files and folders using Terminal commands: ```sh defaults write com.apple.finder AppleShowAllFiles YES Killall Finder // Then, restart the Finder by holding down Option+Control and clicking the Finder icon in the Dock, then choose Relaunch ``` Trackpad Spotlight >> Trackpad >> Point & Click = Check Tap to Click Spotlight >> Trackpad >> Point & Click = Un-check Force Click and Haptic Feedback Spotlight >> Trackpad >> Point & Click = Increase Tracking Speed Keyboard Spotlight >> Keyboard >> Keyboard = Check Use F1,F2 keys as standard keys Spotlight >> Keyboard >> Shortcuts = Double click Show Desktop and hit Cmd+D Desktop Spotlight >> Desktop & Screensavers >> Screen Saver = Check 'Show with clock' Spotlight >> Desktop & Screensavers >> Screen Saver => Hot Corners Left Bottom = Desktop Right Bottom = Screen Saver Locking - Spotlight >> Keychain Access >> Preferences = Check 'Show keychain status in menu bar'. Screen Saver - From Finder >> Go To >> /System/Library/Frameworks/ScreenSaver.framework/Versions/A/Resources/ (Sierra) - Drag and Drop ScreenSaveEngine app on Dock. - High Sierra = /System/Library/CoreServices/ Development Tools & Settings VS Code iTerm2 Sublime SnagIt Spectacle Date-O","title":"Mac Setup"},{"location":"Coding/Setting%20Up%20Mac%20for%20Coding/#mac-tips","text":"Start-up Programs Spotlight >> Users & Groups >> Login Items = Add apps Productivity Tips Command Purpose CMD+Ctrl+F Full Screen Spotlight Find app, use as calculator, dictionary File delete Backspace delete Back Delete del Forward delete fn del Capture Screen CMD+SHIFT+4 Move Up/Down the Folders CMD + Up/Down CMD Full CMD Full","title":"Mac Tips"},{"location":"Coding/Setting%20Up%20the%20Terminal/","text":"Pre-requisites iTerm2 Visual Studio Code brew : /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" Install \"Oh My Zsh\" sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" Install theme Powerlevel10k git clone https://github.com/romkatv/powerlevel10k.git $ZSH_CUSTOM/themes/powerlevel10k code ~/.zshrc ZSH_THEME=\"powerlevel10k/powerlevel10k\" Then save the zshrc file, quit the iTerm2, and re-open it. You will see the Powerlevel10k configuration wizard. You can run the wizard later with the below command on iTerm. pk10 configure Syntax highlighting. brew install zsh-syntax-highlighting source /usr/local/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh","title":"Setting Up the Terminal"},{"location":"Coding/Setting%20Up%20the%20Terminal/#pre-requisites","text":"iTerm2 Visual Studio Code brew : /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" Install \"Oh My Zsh\" sh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" Install theme Powerlevel10k git clone https://github.com/romkatv/powerlevel10k.git $ZSH_CUSTOM/themes/powerlevel10k code ~/.zshrc ZSH_THEME=\"powerlevel10k/powerlevel10k\" Then save the zshrc file, quit the iTerm2, and re-open it. You will see the Powerlevel10k configuration wizard. You can run the wizard later with the below command on iTerm. pk10 configure Syntax highlighting. brew install zsh-syntax-highlighting source /usr/local/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh","title":"Pre-requisites"},{"location":"Personal%20Finance/Home%20Buying/","text":"Terms EMI = Estimated Monthly Installment PMI = Private Mortgage Insurance (if downpayment < 20%) usually ~0.5% HOA = Housing Association, if you buy a house in a community. Downpayment = The % of the total cost. If it's <20%, the buyer needs to pay PMI. Earnest Money Deposit = If buyer is serious and makes an offer, the seller takes his property off the market and demands to pay the earnest money. The Purchase Agreement is prepared with details on the total earnest money deposit and its payment schedule e.g. 5% of the $600k to be paid over 3 installments. If the deal falls through (unforeseen event, loan rejection), the seller may keep the deposit. If the deal goes through, the deposit is counted towards downpayment and/or closing cost. Closing Cost = Loan Cost (origination charges + services borrower shop/did not shop) + Other Cost (tax + fees + initial escrow payment + other) Calculators https://better.com/loan-comparison-calculator Before Buying How much you can afford? EMI = Principle + Interest + PMI + Property Tax + HOA Your EMI should be < 25% of your Monthly Gross Income. (EMIx100)/Gross = ? Monthly Income = Post-tax income - (Home EMI + Car EMI + Grocery + Bills) Mortgage Types Conventional FHA VA 15/30 year Federal Housing Veterans Downpayment 3.5% downpayment No Downpayment PMI if Downpayment < 20% ? No PMI Factors Impacting your mortgage rate Credit Score (Debt/Income ratio, On-time credit payment) Mortgage Rate Types fixed-rate 30/15 year conventional adjustable-rate (ARM) - quoted as fixed-period/adjustable-frequency e.g. 5/6 means interest rate is fixed for first 5 years and may change every 6-months until it is paid off. Risky Loan Balloon Payment - if the loan demands huge payment at the end of the loan term. Prepayment Penalty if you refinance or pay off your loan. Prepayment up to 20% of the balance is usually allowed. Points or Credits 1 point is 1% so 1 point of $100k is $1000 zero-point loan - The lender takes or gives no point. When lender offers some points, it can be used against the closing cost. Sometimes the seller asks to use their preferred lender and doing so, offers some credit in closing cost. If you're planning to stay for longer time, prefer lower interest rate over the credits/points. If you're planning to refinance or resell the house soon, get the credits/points now! Talking to the Salesman/Seller - What are the incentives of using your preferred lender?","title":"Home Buying"},{"location":"Personal%20Finance/Home%20Buying/#terms","text":"EMI = Estimated Monthly Installment PMI = Private Mortgage Insurance (if downpayment < 20%) usually ~0.5% HOA = Housing Association, if you buy a house in a community. Downpayment = The % of the total cost. If it's <20%, the buyer needs to pay PMI. Earnest Money Deposit = If buyer is serious and makes an offer, the seller takes his property off the market and demands to pay the earnest money. The Purchase Agreement is prepared with details on the total earnest money deposit and its payment schedule e.g. 5% of the $600k to be paid over 3 installments. If the deal falls through (unforeseen event, loan rejection), the seller may keep the deposit. If the deal goes through, the deposit is counted towards downpayment and/or closing cost. Closing Cost = Loan Cost (origination charges + services borrower shop/did not shop) + Other Cost (tax + fees + initial escrow payment + other)","title":"Terms"},{"location":"Personal%20Finance/Home%20Buying/#calculators","text":"https://better.com/loan-comparison-calculator","title":"Calculators"},{"location":"Personal%20Finance/Home%20Buying/#before-buying","text":"How much you can afford? EMI = Principle + Interest + PMI + Property Tax + HOA Your EMI should be < 25% of your Monthly Gross Income. (EMIx100)/Gross = ? Monthly Income = Post-tax income - (Home EMI + Car EMI + Grocery + Bills)","title":"Before Buying"},{"location":"Personal%20Finance/Home%20Buying/#mortgage-types","text":"Conventional FHA VA 15/30 year Federal Housing Veterans Downpayment 3.5% downpayment No Downpayment PMI if Downpayment < 20% ? No PMI","title":"Mortgage Types"},{"location":"Personal%20Finance/Home%20Buying/#factors-impacting-your-mortgage-rate","text":"Credit Score (Debt/Income ratio, On-time credit payment)","title":"Factors Impacting your mortgage rate"},{"location":"Personal%20Finance/Home%20Buying/#mortgage-rate-types","text":"fixed-rate 30/15 year conventional adjustable-rate (ARM) - quoted as fixed-period/adjustable-frequency e.g. 5/6 means interest rate is fixed for first 5 years and may change every 6-months until it is paid off.","title":"Mortgage Rate Types"},{"location":"Personal%20Finance/Home%20Buying/#risky-loan","text":"Balloon Payment - if the loan demands huge payment at the end of the loan term. Prepayment Penalty if you refinance or pay off your loan. Prepayment up to 20% of the balance is usually allowed.","title":"Risky Loan"},{"location":"Personal%20Finance/Home%20Buying/#points-or-credits","text":"1 point is 1% so 1 point of $100k is $1000 zero-point loan - The lender takes or gives no point. When lender offers some points, it can be used against the closing cost. Sometimes the seller asks to use their preferred lender and doing so, offers some credit in closing cost. If you're planning to stay for longer time, prefer lower interest rate over the credits/points. If you're planning to refinance or resell the house soon, get the credits/points now!","title":"Points or Credits"},{"location":"Personal%20Finance/Home%20Buying/#talking-to-the-salesmanseller","text":"","title":"Talking to the Salesman/Seller"},{"location":"Personal%20Finance/Home%20Buying/#-what-are-the-incentives-of-using-your-preferred-lender","text":"","title":"- What are the incentives of using your preferred lender?"}]}